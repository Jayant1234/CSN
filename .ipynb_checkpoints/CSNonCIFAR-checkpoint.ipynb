{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24194,
     "status": "ok",
     "timestamp": 1594786821857,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "69SP7G-VqfOl",
    "outputId": "8fe190e2-cbf2-4819-8a56-fd1483a04757"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1594786825134,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "dzoHeNtzcC2J",
    "outputId": "8b098019-4d3a-45de-a099-dfeb14e81e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 15 04:20:23 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9687,
     "status": "ok",
     "timestamp": 1594786835639,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "0U4RIt-pcG5P",
    "outputId": "103b527d-a817-426e-cc66-2c1b2763456f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/67/f403d4ae6e9cd74b546ee88cccdb29b8415a9c1b3d80aebeb20c9ea91d96/pytorch-1.0.2.tar.gz\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Installing collected packages: pytorch\n",
      "    Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-pw58nu3l/pytorch/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-pw58nu3l/pytorch/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-igq04ca2/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
      "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.5.1+cu101)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchvision) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch\n",
    "!pip install torchvision\n",
    "!pip install livelossplot --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10930,
     "status": "ok",
     "timestamp": 1594786839056,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "Gq3nIOp3ngFy"
   },
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torchviz import make_dot\n",
    "from torch.autograd import Variable\n",
    "#from torchsummary import summary #make this work\n",
    "import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "#from graphviz import Digraph\n",
    "import torch.nn.utils.prune as prune\n",
    "import math\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10108,
     "status": "ok",
     "timestamp": 1594786839059,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "hNscMfyYnxqU",
    "outputId": "e014e151-0a13-49e0-a4f6-6a9530b07fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10159,
     "status": "ok",
     "timestamp": 1594786839636,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "ejfRnLgsn6Wy"
   },
   "outputs": [],
   "source": [
    "def get_loaders(batch_size=64,num_workers=10):\n",
    "    std=(0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "    mean=(0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "    transform_train = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "    \n",
    "    train_set, val_set = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    valoader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    return trainloader,valoader,testloader\n",
    "\n",
    "class BaseNetCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A base Network, 81+81+81 parameters in each BaseNet. \n",
    "    \"\"\"\n",
    "    def __init__(self,channels=64):\n",
    "        inp=3\n",
    "        super(BaseNetCNN, self).__init__()\n",
    "        self.channels=channels\n",
    "        kernel_size=3\n",
    "        BatchNorm=nn.BatchNorm2d\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=1, bias=False)\n",
    "        self.bn1 = BatchNorm(channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm(channels)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels, out_channels=channels,kernel_size=kernel_size, padding=1, bias=False)\n",
    "        self.bn3 = BatchNorm(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out += x\n",
    "\n",
    "        return out\n",
    "        \n",
    "# class BaseNetCNN(nn.Module):\n",
    "\n",
    "#     def __init__(self, channels):\n",
    "#         super(BaseNetCNN, self).__init__()\n",
    "#         self.channels=channels\n",
    "#         stride=1\n",
    "#         dilation=1 \n",
    "#         downsample=None\n",
    "#         BatchNorm=nn.BatchNorm2d\n",
    "#         out_channels=channels*2\n",
    "#         self.conv1 = nn.Conv2d(channels, out_channels, kernel_size=1, bias=False)\n",
    "#         self.bn1 = BatchNorm(out_channels)\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride,\n",
    "#                                dilation=dilation, padding=dilation, bias=False)\n",
    "#         self.bn2 = BatchNorm(out_channels)\n",
    "#         self.conv3 = nn.Conv2d(out_channels, channels, kernel_size=1, bias=False)\n",
    "#         self.bn3 = BatchNorm(channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.downsample = downsample\n",
    "#         self.stride = stride\n",
    "#         self.dilation = dilation\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "\n",
    "#         if self.downsample is not None:\n",
    "#             residual = self.downsample(x)\n",
    "\n",
    "#         out += residual\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         return out\n",
    "    \n",
    "# class BaseNetCNN(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A base Network, 81+81+81 parameters in each BaseNet. \n",
    "#     \"\"\"\n",
    "#     def __init__(self,channels=32):\n",
    "#         inp=3\n",
    "#         super(BaseNetCNN, self).__init__()\n",
    "#         self.channels=channels\n",
    "#         kernel_size=3\n",
    "#         BatchNorm=nn.BatchNorm2d\n",
    "#         self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=1, bias=False)\n",
    "#         self.bn1 = BatchNorm(channels)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=1, bias=False)\n",
    "#         self.bn2 = BatchNorm(channels)\n",
    "#         self.conv3 = nn.Conv2d(in_channels=channels, out_channels=channels,kernel_size=kernel_size, padding=1, bias=False)\n",
    "#         self.bn3 = BatchNorm(channels)\n",
    "#         self.conv4 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=1, bias=False)\n",
    "#         self.bn4 = BatchNorm(channels)\n",
    "#         self.conv5 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=1, bias=False)\n",
    "#         self.bn5 = BatchNorm(channels)\n",
    "#         self.conv6 = nn.Conv2d(in_channels=channels, out_channels=channels,kernel_size=kernel_size, padding=1, bias=False)\n",
    "#         self.bn6 = BatchNorm(channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual=x\n",
    "#         out = self.conv1(residual)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         residual += out\n",
    "#         out = self.conv3(residual)\n",
    "#         out = self.bn3(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv4(x)\n",
    "#         out = self.bn4(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         residual += out\n",
    "\n",
    "#         out = self.conv5(residual)\n",
    "#         out = self.bn5(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv6(out)\n",
    "#         out = self.bn6(out)\n",
    "#         out = self.relu(out)\n",
    "        \n",
    "#         residual+=out\n",
    "\n",
    "#         return residual\n",
    "\n",
    "    \n",
    "class InterconnectionCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    1x1 convs interconnect various Modules based on their channel size. \n",
    "    \"\"\"\n",
    "    def __init__(self,cnnsize):\n",
    "        super(InterconnectionCNN, self).__init__()\n",
    "        #newsize\n",
    "        #run one layer or two// just doing it for 2x2 module size. DAMN.   \n",
    "        #one idea here was to divide the input here and make it go through a layer, seems unneccesary\n",
    "        BatchNorm=nn.BatchNorm2d\n",
    "        self.conv1 = nn.Conv2d(in_channels=cnnsize, out_channels=512, kernel_size=1,padding=0, bias=False)\n",
    "        self.bn1 = BatchNorm(512)\n",
    "        self.conv2 = nn.Conv2d(in_channels=512, out_channels=cnnsize, kernel_size=1,padding=0, bias=False)\n",
    "        self.bn2 = BatchNorm(cnnsize)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ModularityCNN(nn.Module):\n",
    "    def __init__(self,args,depth,width,level,basenet_size):\n",
    "        super(ModularityCNN, self).__init__()\n",
    "        \"\"\"\n",
    "        args(list) : list containing all modules to be sewed\n",
    "        depth(int) : The expansion depth\n",
    "        width(int) : The total width = expansion  width**expo_level + linear_width_multiplier*(level-expo_level)\n",
    "        level(int) : Levels-1 above basenet, i.e. it contains info about interconnection no.\n",
    "        //especially needed for weight initlization.   \n",
    "        basenet_size(int): The no of channels in basenet. \n",
    "        \"\"\"\n",
    "\n",
    "        self.depth=depth\n",
    "        self.width=width\n",
    "        self.level=level\n",
    "        self.basenet_size= basenet_size \n",
    "        self.cnn_channels=self.basenet_size*(self.width**self.level) #formula just works till there is no width increase in linear exp.\n",
    "#         print(\"CNN Channel Count: \",self.cnn_channels)\n",
    "#         print(\"Level\",level)\n",
    "        if len(args) != width*depth:\n",
    "            assert \"Error : Module size not right!\"\n",
    "        if type(args) is list:\n",
    "            i=0\n",
    "            j=0\n",
    "            for module in args:  \n",
    "                self.add_module(\"module\"+str(i), module)\n",
    "                i+=1\n",
    "                #add interconnections based on size of depth and width and level \n",
    "                if(i%self.width==0 and self.width>1 and i<((self.width*self.depth)-1)):\n",
    "                    self.add_module(\"interconnection\"+str(j),InterconnectionCNN(self.cnn_channels))\n",
    "                    j+=1\n",
    "        else:#ordered Dict case/ unused\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, x): \n",
    "        foro=[]\n",
    "        fori=[]\n",
    "#         print(\"Level\", self.level, \"First X split Sections and x.size():\",int(list(x.size())[1]/self.width),x.size())\n",
    "#         print(\"Split 2's size\",fori[1].size())\n",
    "        fori=torch.split(x,int(list(x.size())[1]/self.width), dim=1)\n",
    "        module_iterator= self.children()\n",
    "\n",
    "        for i in range(self.depth): \n",
    "            if(i==0):\n",
    "                for j in range(self.width): \n",
    "                    #first layer in given width\n",
    "                    depthbuff=0\n",
    "                    if(j==0):\n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                    elif(j==self.width-1): \n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                    else:   \n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                \n",
    "            elif(i==self.depth-1): \n",
    "                module=next(module_iterator)# this is interconnection \n",
    "                #print(module)\n",
    "                #print(\"LEVEL:\",self.level,\"Above module's concatenated input size: \", torch.cat(foro,1).size())\n",
    "                inp=torch.cat(foro,1)\n",
    "                output=module(inp)\n",
    "                del inp\n",
    "                del fori\n",
    "                fori=torch.split(output,int(list(output.size())[1]/self.width),1)\n",
    "                del output\n",
    "                depthbuff=0\n",
    "                del foro\n",
    "                foro=[]\n",
    "                for j in range(self.width): \n",
    "                    if(j==0):\n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                    elif(j==self.width-1): \n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                    else:   \n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "            else:\n",
    "                module=next(module_iterator)# this is interconnection hopefully\n",
    "#                 print(module)\n",
    "#                 print(\"LEVEL:\",self.level,\"Above module's concatenated input size: \", torch.cat(foro,1).size())\n",
    "                inp=torch.cat(foro,1)\n",
    "                output=module(inp)\n",
    "                del inp\n",
    "                del fori\n",
    "                fori=torch.split(output,int(list(output.size())[1]/self.width),1)\n",
    "                del output\n",
    "                depthbuff=0\n",
    "                del foro\n",
    "                foro=[]\n",
    "                for j in range(self.width): \n",
    "                    \n",
    "                    if(j==0):\n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                    elif(j==self.width-1): \n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "                    else:   \n",
    "                        module=next(module_iterator)\n",
    "                        out1=module(fori[depthbuff])\n",
    "                        foro.append(out1)\n",
    "                        depthbuff+=1\n",
    "\n",
    "        out= torch.cat(foro,1)\n",
    "        del foro\n",
    "        out += x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10128,
     "status": "ok",
     "timestamp": 1594786840134,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "zFbW-wsEoBDM"
   },
   "outputs": [],
   "source": [
    "class CompositionalSparseNetCNN(nn.Module):\n",
    "    def __init__(self,inp=3,stage=\"Base_model\",exp_level=4,channels_width=256, exponential_cutoff=4,\\\n",
    "                 dir_saved_weights=r'Weights/',expo_depth=2,expo_width=2,nettype=\"CNN\",prev_path=\"CSN\"):\n",
    "        \"\"\"\n",
    "        inp[int]: baseNet size\n",
    "        pruning_type (string) : 'unstructured' or 'structured', structured refers to layers or modules\n",
    "        stage (string) : 'base_model' or exp_expansion' or 'linear_expansion' or 'reload'\n",
    "        exp_level (int) : 0-100 : represents total expansion  level including both exponen. and linear.\n",
    "        exponential_cutoff(int) : 1 to 20, tells expansion level at which exponential expansion halts\n",
    "        dir_saved_weights (string): filepath to save prev. modular hierarchy level weights for uploading\n",
    "        expo_depth (int) : exponential expansion depth size\n",
    "        expo_width (int) : exponential expansion width size\n",
    "        channels_width(int) : Based on exponential expansion size, this width will change!\n",
    "        nettype (string) : \"CNN\" or \"FCN\" \n",
    "        \"\"\"\n",
    "\n",
    "        super(CompositionalSparseNetCNN, self).__init__()\n",
    "        self.inp=inp\n",
    "        self.exp_level=exp_level\n",
    "        self.stage=stage\n",
    "        self.dir_saved_weights=dir_saved_weights\n",
    "        self.exponential_cutoff=exponential_cutoff\n",
    "        self.prev_path=prev_path\n",
    "        self.nettype=nettype\n",
    "        \n",
    "        \n",
    "        if(self.stage=='exp_expansion' and (self.exp_level!=self.exponential_cutoff)):\n",
    "            assert \"Inconsistent Model configuration, At exponentially expansion stage, expansion level should be the same number as exponential exp level\"\n",
    "    \n",
    "        #merge both exponential and linear expansion methods after weight manipulation.\n",
    "        #exponential expansion\n",
    "        self.expo_depth=expo_depth # y=(ed)^level y= x^a\n",
    "        self.expo_width=expo_width\n",
    "        #linear expansion\n",
    "        self.linear_depth=4 # y= multiplier* ld #y=mx \n",
    "        self.linear_width=0 #lets keep this constant for a while because of GPU limitations.  \n",
    "        self.multiplier=exp_level-self.exponential_cutoff\n",
    "        self.basenet_size= int(channels_width/(self.expo_width**self.exponential_cutoff + self.linear_width*self.multiplier))\n",
    "        print(self.basenet_size)\n",
    "        #first two layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.channels_width=channels_width\n",
    "        self.downsamp1= nn.Sequential(\n",
    "            nn.Conv2d(3,self.channels_width, kernel_size=2, padding=0,stride=2, bias=False),\n",
    "            nn.BatchNorm2d(channels_width),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.channels_width,self.channels_width, kernel_size=3, padding=1,stride=1, bias=False),\n",
    "            nn.BatchNorm2d(channels_width),\n",
    "            nn.ReLU(inplace=True)\n",
    "            )\n",
    "        #transfer contents to a method. \n",
    "        total_width = self.expo_width**exponential_cutoff + (exp_level-exponential_cutoff)*self.linear_width\n",
    "        all_modules=self.expansion()\n",
    "        #modularityCNN definition\n",
    "        if(self.multiplier>0): \n",
    "            #perform addition of modules similar to above!. \n",
    "            td= self.expo_depth+self.linear_depth*self.multiplier\n",
    "            tw=self.expo_width+self.linear_width*self.multiplier\n",
    "            self.modularity=ModularityCNN(all_modules,td,tw,self.exponential_cutoff,self.basenet_size)\n",
    "        else :\n",
    "            self.modularity=ModularityCNN(all_modules,self.expo_depth,self.expo_width,self.exponential_cutoff,self.basenet_size)\n",
    "        if(self.stage=='linear_expansion'): \n",
    "            #loading dicts from previous expansion levels. \n",
    "            #PUT IN A CHECK IF LEVEL 1 IS NOT THERE, JUST PUT EXPODICT AS rand weight init\n",
    "            path=self.prev_path+'/'+'level'+str(self.exponential_cutoff-1)+'.pth'\n",
    "            statedict=torch.load(path)     \n",
    "            expodict={}\n",
    "            for k in list(statedict.keys()):\n",
    "                lst=k.split('.')\n",
    "                if(lst[0]==\"modularity\"):\n",
    "                    lst=lst[1:]\n",
    "                    new_k='.'.join(lst)\n",
    "                    expodict[new_k]=statedict[k]\n",
    "            \n",
    "            old_depth=self.expo_depth + self.linear_depth*(self.multiplier-1)\n",
    "            old_width=self.expo_width + self.linear_width*(self.multiplier-1)\n",
    "            new_depth=self.expo_depth + self.linear_depth*self.multiplier\n",
    "            new_width=self.expo_width + self.linear_width*self.multiplier\n",
    "            \n",
    "            path=self.prev_path+'/'+'level'+str(self.exp_level-1)+'.pth'\n",
    "            statedict=torch.load(path)     \n",
    "            lineardict= [{}] * (old_depth*old_width)\n",
    "            linearinter=[{}] * (old_depth-1)\n",
    "            for k in list(statedict.keys()):\n",
    "                lst=k.split('.')\n",
    "                if(lst[0]==\"modularity\"):\n",
    "                    lst=lst[1:]\n",
    "                    no=0\n",
    "                    if lst[0][0]==\"m\": \n",
    "                        no=int(lst[0][6:])#getting no. from module..no\n",
    "                        new_k='.'.join(lst[1:])\n",
    "                        lineardict[no][new_k]=statedict[k] \n",
    "                    elif lst[0][0]==\"i\": \n",
    "                        no=int(lst[0][15:])#getting no. from interconnection..no\n",
    "                        new_k='.'.join(lst[1:])\n",
    "                        linearinter[no][new_k]=statedict[k]\n",
    "                    else : \n",
    "                        assert \"The loaded linear dict is not right!\"\n",
    "                    \n",
    "        \n",
    "            #print('lineardict', [dit.keys() for dit in lineardict])\n",
    "            #print('linearinter',linearinter)\n",
    "\n",
    "            width_pos=math.ceil(float(new_width - old_width)/2)\n",
    "            depth_pos=math.ceil(float(new_depth - old_depth)/2)\n",
    "            first_num=depth_pos*new_width + width_pos\n",
    "            total_lst=[i for i in range(0,new_depth*new_width)]\n",
    "            lst_linear=[]\n",
    "            lst_exp=[]\n",
    "            #can find interconnection no. based on depth_pos simply. Its depth_pos-1\n",
    "            \n",
    "            for j in range(0,old_depth):\n",
    "                lst_linear.extend([i+first_num+j*new_width for i in range(0,old_width)])\n",
    "            print(lst_linear)\n",
    "            lst_exp=[item for item in total_lst if item not in lst_linear]\n",
    "            print(lst_exp)\n",
    "            lin_buffer=0\n",
    "            for n,module in enumerate(self.modularity.named_children()):\n",
    "                name=module[0]\n",
    "                mtype= \"x\"\n",
    "                no=0\n",
    "                if name[0]==\"m\": \n",
    "                    no=int(name[6:])#getting no. from module..no\n",
    "                    mtype=\"m\"\n",
    "                elif name[0]==\"i\": \n",
    "                    no=int(name[15:])#getting no. from interconnection..no\n",
    "                    mtype=\"i\"\n",
    "                else : \n",
    "                    no=None\n",
    "                    print(no,name)\n",
    "                    print(mtype)\n",
    "                    continue\n",
    "                print(no,name)\n",
    "                print(mtype)  \n",
    "                \n",
    "                if(mtype==\"m\"):\n",
    "                    if (no in lst_linear):\n",
    "                        module[1].load_state_dict(lineardict[lin_buffer])\n",
    "                        lin_buffer+=1\n",
    "                        print(\"Checking linBuffer and module no./decrease intercon count\",lin_buffer,n)\n",
    "                    elif no in lst_exp: \n",
    "                        module[1].load_state_dict(expodict)\n",
    "                elif(mtype==\"i\") and (old_width==new_width):\n",
    "                    #there is no way in current code interconnections are gonna work through transfer learning if width is increased.\n",
    "                    inter_num=int(((lin_buffer)/old_width)-1)\n",
    "                    if(0 < lin_buffer < max(lst_linear)) and inter_num < len(linearinter):\n",
    "                        print(\"Checking internum and module no./decrease intercon count\",inter_num,n)\n",
    "                        module[1].load_state_dict(linearinter[inter_num]) \n",
    "\n",
    "                else : \n",
    "                    assert \"What kind of module is this? it should not be here.\"\n",
    "        #print(\"self.modularity.state_dict: \",self.modularity.state_dict().keys())\n",
    "\n",
    "        self.downsamp2 = nn.Conv2d(channels_width, 16, kernel_size=3, padding=1,stride=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        num_classes=10\n",
    "        self.classifier=nn.Linear(16*16*16, num_classes)\n",
    "    \n",
    "    def expansion(self): \n",
    "        \"\"\"\n",
    "        creates a basenet list and stiches them into Modularity Classes design. \n",
    "        \"\"\"\n",
    "        \n",
    "        exp_basenet_list = [BaseNetCNN(self.basenet_size) for count in range(0,(self.expo_depth*self.expo_width)**self.exponential_cutoff)]\n",
    "        module_size=self.expo_depth*self.expo_width\n",
    "        for i in range(0,self.exponential_cutoff-1):\n",
    "            new_basenet=[]\n",
    "            for j in range(0,(module_size)**(self.exponential_cutoff-i),module_size):\n",
    "                new_basenet.append(ModularityCNN(exp_basenet_list[j:j+module_size],self.expo_depth,self.expo_width,i+1,self.basenet_size))\n",
    "            exp_basenet_list=new_basenet\n",
    "\n",
    "        if(self.stage =='exp_expansion'): \n",
    "            for i in range(len(exp_basenet_list)):\n",
    "                path=self.prev_path+'/'+'level'+str(self.exponential_cutoff-1)+'.pth'\n",
    "                statedict=torch.load(path)              \n",
    "                #In order to load this\n",
    "                \"\"\"I just realized I have to write this script in weight init method or it will be just overwritten right\n",
    "                ??\"\"\"\n",
    "                modulardict={}\n",
    "                for k in list(statedict.keys()):\n",
    "                    lst=k.split('.')\n",
    "                    if(lst[0]==\"modularity\"):\n",
    "                        lst=lst[1:]\n",
    "                        new_k='.'.join(lst)\n",
    "                        modulardict[new_k]=statedict[k]\n",
    "                exp_basenet_list[i].load_state_dict(modulardict)\n",
    "                #print(exp_basenet_list[i].state_dict())\n",
    "\n",
    "        lin_basenet_list=[]\n",
    "        #linear expansion\n",
    "        if(self.multiplier>0): \n",
    "            #td = total depth of last level of dynamic hierarchy \n",
    "            #tw =  total width of last level of dynamic hierarchy\n",
    "            td= self.expo_depth+self.linear_depth*self.multiplier\n",
    "            tw=self.expo_width+self.linear_width*self.multiplier\n",
    "            d=self.expo_depth\n",
    "            w=self.expo_width\n",
    "            lin_basenet_list = [BaseNetCNN(self.basenet_size) for count in range(0,((d*w)**(self.exponential_cutoff-1))*(td*tw-d*w))]\n",
    "        \n",
    "            module_size=d*w\n",
    "            final_module_size=td*tw\n",
    "            for i in range(0,self.exponential_cutoff-1):\n",
    "                new_basenet=[]\n",
    "                j=0\n",
    "                while(j<len(lin_basenet_list)):\n",
    "                    new_basenet.append(ModularityCNN(lin_basenet_list[j:j+module_size],self.expo_depth,self.expo_width,i+1,self.basenet_size))\n",
    "                    j+=module_size                        \n",
    "                lin_basenet_list=new_basenet\n",
    "\n",
    "        basenet_list= exp_basenet_list+lin_basenet_list\n",
    "        return basenet_list\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"based on hierarchy levels, depths for linear and exponential expansion, \n",
    "        base net is repeated in init with various self.basenet in list and then used to be trained.\"\"\"\n",
    "        x=self.downsamp1(x)\n",
    "        x=self.modularity(x)\n",
    "        x=self.downsamp2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.relu(x)\n",
    "        x=x.view(-1,16*16*16)\n",
    "        x=self.classifier(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9706,
     "status": "ok",
     "timestamp": 1594786840135,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "8Z5qHlKvoM18"
   },
   "outputs": [],
   "source": [
    "def global_pruning(model,pruning_rate=0.3):\n",
    "        \"\"\"\n",
    "        Global unstructured magnitude L1 norm pruning\n",
    "        design or experiment choices : to create pruning on top of pruning or remove old ones to produce new higher rates? \n",
    "        Current : Keeps pruning by keeping weights zero! but next pruning will just prune these zeroes too right, need to increase\n",
    "        the rate still, so the other choice is actually better. To just restore everything back to normal. shall do that then\n",
    "        \"\"\"\n",
    "        parameters_to_prune={}\n",
    "        for name, param in model.named_parameters():\n",
    "            #parameters_to_prune[name]= param.data\n",
    "            lst=name.split('.')            \n",
    "            s=\".\"\n",
    "            LOC=s.join([\"model\"]+lst[0:-1])\n",
    "            if(lst[0]=='modularity'):\n",
    "                exec(\"parameters_to_prune[\"+LOC+\"]= 'weight'\")#just pruning weights, not biases/thats how its done.\n",
    "                \n",
    "        parameter_tuple=tuple(list(parameters_to_prune.items()))\n",
    "        prune.global_unstructured(\n",
    "             parameter_tuple,\n",
    "             pruning_method=prune.L1Unstructured,\n",
    "             amount=pruning_rate,)\n",
    "\n",
    "def remove_global_pruning(model): \n",
    "        parameters_to_prune={}\n",
    "        for name, param in model.named_parameters():\n",
    "            #parameters_to_prune[name]= param.data\n",
    "            lst=name.split('.')            \n",
    "            s=\".\"\n",
    "            LOC=s.join([\"model\"]+lst[0:-1])\n",
    "            if(lst[0]=='modularity'):\n",
    "                exec(\"parameters_to_prune[\"+LOC+\"]= 'weight'\")#just pruning weights, not biases/thats how its done.\n",
    "        \n",
    "        parameter_tuple=tuple(list(parameters_to_prune.items())) \n",
    "        for m in parameter_tuple: \n",
    "            #does the job really, removed all pruning reparametrization but keeps zeroes in weights. \n",
    "            #print(m)\n",
    "            prune.remove(m[0], 'weight')\n",
    "            \n",
    "def structured_pruning(model, pruning_rate=0.3): \n",
    "    \"\"\"Acts on all BaseNets differently!\"\"\"\n",
    "    basenets_to_prune=[]\n",
    "    for name, param in model.named_parameters():\n",
    "        #parameters_to_prune[name]= param.data\n",
    "        lst=name.split('.')            \n",
    "        s=\".\"\n",
    "        LOC=s.join([\"model\"]+lst[0:-1])\n",
    "        #print(lst[-2])\n",
    "        if(lst[0]=='modularity') and lst[-2][0:4]=='conv':\n",
    "            basenets_to_prune.append(LOC)\n",
    "            #print(LOC)\n",
    "    basenets_to_prune=list(set(basenets_to_prune))\n",
    "    for basenets in basenets_to_prune:\n",
    "        exec(\"prune.ln_structured(\"+basenets+\", name='weight', amount=\"+str(pruning_rate)+\", n=2, dim=0)\")\n",
    "\n",
    "def remove_structured_pruning(model): \n",
    "    \"\"\"Acts on all BaseNets differently!\"\"\"\n",
    "    basenets_to_prune=[]\n",
    "    for name, param in model.named_parameters():\n",
    "        #parameters_to_prune[name]= param.data\n",
    "        lst=name.split('.')            \n",
    "        s=\".\"\n",
    "        LOC=s.join([\"model\"]+lst[0:-1])\n",
    "        #print(lst[-2])\n",
    "        if(lst[0]=='modularity') and lst[-2][0:4]=='conv':\n",
    "            basenets_to_prune.append(LOC)\n",
    "            print(LOC)\n",
    "    basenets_to_prune=list(set(basenets_to_prune))\n",
    "    for basenets in basenets_to_prune: \n",
    "        exec(\"prune.remove(\"+basenets+\",'weight')\")\n",
    "\n",
    "\n",
    "def randomize_pruning(model,ranmodel):\n",
    "    \"\"\"WORKS do it only for modularity though. Although 0.8*random1 + 0.2*random2 is still random..!\"\"\"\n",
    "    beta = 0.3 #The interpolation parameter    \n",
    "    dict1=model.modularity.state_dict()\n",
    "    dict2=ranmodel.modularity.state_dict()\n",
    "    dict3={}\n",
    "    #print(dict1)\n",
    "    #print(dict1.keys())\n",
    "    for key in dict1.keys(): \n",
    "        dict3[key]=beta*(dict2[key])+(1-beta)*dict1[key]\n",
    "    #print(dict3)\n",
    "    model.modularity.load_state_dict(dict3)\n",
    "    return model\n",
    "            \n",
    "    #parameter_tuple=tuple(list(parameters_to_prune.items()))\n",
    "    #print(parameter_tuple)\n",
    "    #for item in parameter_tuple: \n",
    "        \n",
    "def create_graph(model,inp):\n",
    "    nput = torch.randn(1, *inp)\n",
    "    #transforms = [\n",
    "        # Fold Conv, BN, RELU layers into one\n",
    "        #hl.transforms.Fold(\"Transpose > MatMul > Add > Relu\", \"FCLayer\",name=\"FCLayer\"),\n",
    "        # Fold repeated blocks\n",
    "       #  hl.transforms.FoldDuplicates(),]\n",
    "#     hl.build_graph(model, nput,transforms=transforms).save(\"newone\",format=\"pdf\")\n",
    "    hl.build_graph(model, nput).save(\"newone\",format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1594787004786,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "qHKlXNEkp8o9"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='max', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)\n",
    "\n",
    "def train(model,trainloader,valloader, epochs=30,accumulation_size=1):     \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    es = EarlyStopping(patience=10)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "      running_loss = 0.0\n",
    "      iter_loss=0.0\n",
    "      model.train()\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "          # get the inputs; data is a list of [inputs, labels]\n",
    "          inputs, labels = data\n",
    "          inputs=inputs.cuda()\n",
    "          inputs=inputs.view(-1,3,32,32)\n",
    "          #print(inputs.size())\n",
    "          labels=labels.cuda()\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "          # forward + backward + optimize\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels).cuda()\n",
    "          loss.backward()\n",
    "          clipping_value = 1 # arbitrary value of your choosing\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "          optimizer.step()\n",
    "\n",
    "          # print statistics\n",
    "          l=loss.item()\n",
    "          running_loss += l\n",
    "          iter_loss+=l\n",
    "          if(i%200==0):\n",
    "            print('[%d, %5d] iter loss: %.3f' % (epoch + 1, i + 1, iter_loss))\n",
    "          iter_loss=0.0\n",
    "      # for name, param in model.named_parameters():\n",
    "      #   if name[-6:] == 'weight':\n",
    "      #     print('===========\\ngradient:{}\\n----------\\n{}'.format(name,param.grad))\n",
    "      #     break\n",
    "          \n",
    "      # print('[%d] total epoch loss: %.3f' % (epoch + 1, running_loss))\n",
    "      metric = validation(model,valloader)  # evalution on dev set (i.e., holdout from training)\n",
    "      metric=torch.tensor(metric)\n",
    "      print('Val Accuracy:', metric)\n",
    "      # if es.step(metric):\n",
    "      #   print(\"Early stopping is done after val accuracy=\", metric)\n",
    "      #   break  # early stop criterion is met, we can stop now\n",
    "    print('Finished Training')\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.ones_(m.weight)\n",
    "        \n",
    "        \n",
    "def init_kaim(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight,mode='fan_out')\n",
    "    elif isinstance(m,nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight,mode='fan_in')\n",
    "def validation(model, valoader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valoader:\n",
    "            inputs, labels = data\n",
    "            inputs=inputs.cuda()\n",
    "            images=inputs.view(-1,3,32,32)\n",
    "            labels=labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(torch.max(outputs.data, 1))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return(100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3a879c4707ec4572a3534b446a225a5b",
      "e6514d71493647f6aa6ebe6ec373966b",
      "2e51336dfd454e3381fecc7277f3073d",
      "2778a85518ca4663bb3ab32803e5a4cc",
      "32645aafcb794fa4bb3f55b5655228fe",
      "d0825529a2e1439281a0eb5a8fb2821a",
      "2b523bdd71034edbb975ebdb1669bc9c",
      "05060518905f47e0811be4f9902085d6"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 76329094,
     "status": "error",
     "timestamp": 1594863336090,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "4LD9TEYPow0p",
    "outputId": "eb443cad-ff11-48f7-f260-ec2f454a66ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a879c4707ec4572a3534b446a225a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "64\n",
      "\n",
      "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[0, 1, 2, 3, 16, 17, 18, 19]\n",
      "0 module0\n",
      "m\n",
      "1 module1\n",
      "m\n",
      "0 interconnection0\n",
      "i\n",
      "2 module2\n",
      "m\n",
      "3 module3\n",
      "m\n",
      "1 interconnection1\n",
      "i\n",
      "4 module4\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 1 6\n",
      "5 module5\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 2 7\n",
      "2 interconnection2\n",
      "i\n",
      "Checking internum and module no./decrease intercon count 0 8\n",
      "6 module6\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 3 9\n",
      "7 module7\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 4 10\n",
      "3 interconnection3\n",
      "i\n",
      "Checking internum and module no./decrease intercon count 1 11\n",
      "8 module8\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 5 12\n",
      "9 module9\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 6 13\n",
      "4 interconnection4\n",
      "i\n",
      "Checking internum and module no./decrease intercon count 2 14\n",
      "10 module10\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 7 15\n",
      "11 module11\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 8 16\n",
      "5 interconnection5\n",
      "i\n",
      "Checking internum and module no./decrease intercon count 3 17\n",
      "12 module12\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 9 18\n",
      "13 module13\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 10 19\n",
      "6 interconnection6\n",
      "i\n",
      "Checking internum and module no./decrease intercon count 4 20\n",
      "14 module14\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 11 21\n",
      "15 module15\n",
      "m\n",
      "Checking linBuffer and module no./decrease intercon count 12 22\n",
      "7 interconnection7\n",
      "i\n",
      "16 module16\n",
      "m\n",
      "17 module17\n",
      "m\n",
      "8 interconnection8\n",
      "i\n",
      "18 module18\n",
      "m\n",
      "19 module19\n",
      "m\n",
      "Total trainable parameters: 14570026\n",
      "[1,     1] iter loss: 2.435\n",
      "[1,   201] iter loss: 2.093\n",
      "[1,   401] iter loss: 2.046\n",
      "[1,   601] iter loss: 1.783\n",
      "Val Accuracy: tensor(33.0300)\n",
      "[2,     1] iter loss: 1.848\n",
      "[2,   201] iter loss: 1.759\n",
      "[2,   401] iter loss: 1.773\n",
      "[2,   601] iter loss: 1.670\n",
      "Val Accuracy: tensor(37.2000)\n",
      "[3,     1] iter loss: 1.701\n",
      "[3,   201] iter loss: 1.728\n",
      "[3,   401] iter loss: 1.617\n",
      "[3,   601] iter loss: 1.618\n",
      "Val Accuracy: tensor(40.9300)\n",
      "[4,     1] iter loss: 1.615\n",
      "[4,   201] iter loss: 1.658\n",
      "[4,   401] iter loss: 1.503\n",
      "[4,   601] iter loss: 1.627\n",
      "Val Accuracy: tensor(43.7800)\n",
      "[5,     1] iter loss: 1.582\n",
      "[5,   201] iter loss: 1.555\n",
      "[5,   401] iter loss: 1.686\n",
      "[5,   601] iter loss: 1.349\n",
      "Val Accuracy: tensor(44.9100)\n",
      "[6,     1] iter loss: 1.540\n",
      "[6,   201] iter loss: 1.523\n",
      "[6,   401] iter loss: 1.666\n",
      "[6,   601] iter loss: 1.393\n",
      "Val Accuracy: tensor(46.6700)\n",
      "[7,     1] iter loss: 1.559\n",
      "[7,   201] iter loss: 1.408\n",
      "[7,   401] iter loss: 1.663\n",
      "[7,   601] iter loss: 1.405\n",
      "Val Accuracy: tensor(47.2700)\n",
      "[8,     1] iter loss: 1.471\n",
      "[8,   201] iter loss: 1.273\n",
      "[8,   401] iter loss: 1.249\n",
      "[8,   601] iter loss: 1.437\n",
      "Val Accuracy: tensor(49.5300)\n",
      "[9,     1] iter loss: 1.359\n",
      "[9,   201] iter loss: 1.502\n",
      "[9,   401] iter loss: 1.379\n",
      "[9,   601] iter loss: 1.409\n",
      "Val Accuracy: tensor(49.6700)\n",
      "[10,     1] iter loss: 1.258\n",
      "[10,   201] iter loss: 1.195\n",
      "[10,   401] iter loss: 1.519\n",
      "[10,   601] iter loss: 1.166\n",
      "Val Accuracy: tensor(50.5800)\n",
      "[11,     1] iter loss: 1.384\n",
      "[11,   201] iter loss: 1.283\n",
      "[11,   401] iter loss: 1.498\n",
      "[11,   601] iter loss: 1.183\n",
      "Val Accuracy: tensor(50.7500)\n",
      "[12,     1] iter loss: 1.157\n",
      "[12,   201] iter loss: 1.533\n",
      "[12,   401] iter loss: 1.238\n",
      "[12,   601] iter loss: 1.347\n",
      "Val Accuracy: tensor(51.4700)\n",
      "[13,     1] iter loss: 1.360\n",
      "[13,   201] iter loss: 1.131\n",
      "[13,   401] iter loss: 1.276\n",
      "[13,   601] iter loss: 1.137\n",
      "Val Accuracy: tensor(52.3100)\n",
      "[14,     1] iter loss: 1.353\n",
      "[14,   201] iter loss: 1.232\n",
      "[14,   401] iter loss: 1.508\n",
      "[14,   601] iter loss: 1.459\n",
      "Val Accuracy: tensor(53.4700)\n",
      "[15,     1] iter loss: 1.225\n",
      "[15,   201] iter loss: 1.209\n",
      "[15,   401] iter loss: 1.292\n",
      "[15,   601] iter loss: 1.199\n",
      "Val Accuracy: tensor(53.9200)\n",
      "[16,     1] iter loss: 1.314\n",
      "[16,   201] iter loss: 1.285\n",
      "[16,   401] iter loss: 1.088\n",
      "[16,   601] iter loss: 1.238\n",
      "Val Accuracy: tensor(54.2300)\n",
      "[17,     1] iter loss: 1.138\n",
      "[17,   201] iter loss: 1.347\n",
      "[17,   401] iter loss: 1.222\n",
      "[17,   601] iter loss: 1.362\n",
      "Val Accuracy: tensor(54.1000)\n",
      "[18,     1] iter loss: 1.399\n",
      "[18,   201] iter loss: 1.387\n",
      "[18,   401] iter loss: 1.390\n",
      "[18,   601] iter loss: 1.394\n",
      "Val Accuracy: tensor(54.9400)\n",
      "[19,     1] iter loss: 1.293\n",
      "[19,   201] iter loss: 1.219\n",
      "[19,   401] iter loss: 1.290\n",
      "[19,   601] iter loss: 1.112\n",
      "Val Accuracy: tensor(55.7700)\n",
      "[20,     1] iter loss: 1.203\n",
      "[20,   201] iter loss: 1.224\n",
      "[20,   401] iter loss: 1.340\n",
      "[20,   601] iter loss: 1.345\n",
      "Val Accuracy: tensor(56.2400)\n",
      "[21,     1] iter loss: 1.067\n",
      "[21,   201] iter loss: 1.144\n",
      "[21,   401] iter loss: 1.406\n",
      "[21,   601] iter loss: 1.188\n",
      "Val Accuracy: tensor(56.1900)\n",
      "[22,     1] iter loss: 1.320\n",
      "[22,   201] iter loss: 1.110\n",
      "[22,   401] iter loss: 1.222\n",
      "[22,   601] iter loss: 1.151\n",
      "Val Accuracy: tensor(56.5500)\n",
      "[23,     1] iter loss: 1.121\n",
      "[23,   201] iter loss: 1.237\n",
      "[23,   401] iter loss: 1.567\n",
      "[23,   601] iter loss: 1.052\n",
      "Val Accuracy: tensor(57.0800)\n",
      "[24,     1] iter loss: 1.155\n",
      "[24,   201] iter loss: 1.177\n",
      "[24,   401] iter loss: 1.028\n",
      "[24,   601] iter loss: 1.176\n",
      "Val Accuracy: tensor(57.0200)\n",
      "[25,     1] iter loss: 1.183\n",
      "[25,   201] iter loss: 0.915\n",
      "[25,   401] iter loss: 1.158\n",
      "[25,   601] iter loss: 1.292\n",
      "Val Accuracy: tensor(58.0300)\n",
      "[26,     1] iter loss: 1.123\n",
      "[26,   201] iter loss: 1.141\n",
      "[26,   401] iter loss: 1.278\n",
      "[26,   601] iter loss: 1.314\n",
      "Val Accuracy: tensor(57.7300)\n",
      "[27,     1] iter loss: 1.182\n",
      "[27,   201] iter loss: 1.116\n",
      "[27,   401] iter loss: 1.108\n",
      "[27,   601] iter loss: 1.301\n",
      "Val Accuracy: tensor(57.9400)\n",
      "[28,     1] iter loss: 0.969\n",
      "[28,   201] iter loss: 1.131\n",
      "[28,   401] iter loss: 1.193\n",
      "[28,   601] iter loss: 1.033\n",
      "Val Accuracy: tensor(58.9600)\n",
      "[29,     1] iter loss: 1.179\n",
      "[29,   201] iter loss: 1.093\n",
      "[29,   401] iter loss: 1.260\n",
      "[29,   601] iter loss: 1.291\n",
      "Val Accuracy: tensor(59.0900)\n",
      "[30,     1] iter loss: 1.396\n",
      "[30,   201] iter loss: 0.992\n",
      "[30,   401] iter loss: 1.150\n",
      "[30,   601] iter loss: 1.105\n",
      "Val Accuracy: tensor(60.2700)\n",
      "[31,     1] iter loss: 1.085\n",
      "[31,   201] iter loss: 1.095\n",
      "[31,   401] iter loss: 1.146\n",
      "[31,   601] iter loss: 1.071\n",
      "Val Accuracy: tensor(59.2600)\n",
      "[32,     1] iter loss: 0.938\n",
      "[32,   201] iter loss: 1.472\n",
      "[32,   401] iter loss: 1.319\n",
      "[32,   601] iter loss: 0.962\n",
      "Val Accuracy: tensor(59.8100)\n",
      "[33,     1] iter loss: 1.068\n",
      "[33,   201] iter loss: 0.926\n",
      "[33,   401] iter loss: 0.908\n",
      "[33,   601] iter loss: 1.128\n",
      "Val Accuracy: tensor(59.0400)\n",
      "[34,     1] iter loss: 0.927\n",
      "[34,   201] iter loss: 1.341\n",
      "[34,   401] iter loss: 1.120\n",
      "[34,   601] iter loss: 1.002\n",
      "Val Accuracy: tensor(60.3500)\n",
      "[35,     1] iter loss: 1.410\n",
      "[35,   201] iter loss: 0.988\n",
      "[35,   401] iter loss: 1.026\n",
      "[35,   601] iter loss: 0.910\n",
      "Val Accuracy: tensor(60.8700)\n",
      "[36,     1] iter loss: 0.957\n",
      "[36,   201] iter loss: 1.092\n",
      "[36,   401] iter loss: 0.895\n",
      "[36,   601] iter loss: 1.479\n",
      "Val Accuracy: tensor(60.5100)\n",
      "[37,     1] iter loss: 1.104\n",
      "[37,   201] iter loss: 1.099\n",
      "[37,   401] iter loss: 1.231\n",
      "[37,   601] iter loss: 1.043\n",
      "Val Accuracy: tensor(61.2000)\n",
      "[38,     1] iter loss: 1.028\n",
      "[38,   201] iter loss: 0.998\n",
      "[38,   401] iter loss: 0.944\n",
      "[38,   601] iter loss: 0.898\n",
      "Val Accuracy: tensor(61.7700)\n",
      "[39,     1] iter loss: 0.918\n",
      "[39,   201] iter loss: 1.093\n",
      "[39,   401] iter loss: 1.180\n",
      "[39,   601] iter loss: 1.021\n",
      "Val Accuracy: tensor(62.4100)\n",
      "[40,     1] iter loss: 1.153\n",
      "[40,   201] iter loss: 1.238\n",
      "[40,   401] iter loss: 0.950\n",
      "[40,   601] iter loss: 1.126\n",
      "Val Accuracy: tensor(61.1600)\n",
      "[41,     1] iter loss: 1.208\n",
      "[41,   201] iter loss: 1.184\n",
      "[41,   401] iter loss: 1.158\n",
      "[41,   601] iter loss: 1.038\n",
      "Val Accuracy: tensor(62.0800)\n",
      "[42,     1] iter loss: 0.794\n",
      "[42,   201] iter loss: 1.019\n",
      "[42,   401] iter loss: 0.991\n",
      "[42,   601] iter loss: 1.015\n",
      "Val Accuracy: tensor(62.4600)\n",
      "[43,     1] iter loss: 0.879\n",
      "[43,   201] iter loss: 0.983\n",
      "[43,   401] iter loss: 1.163\n",
      "[43,   601] iter loss: 1.025\n",
      "Val Accuracy: tensor(62.4400)\n",
      "[44,     1] iter loss: 0.916\n",
      "[44,   201] iter loss: 1.057\n",
      "[44,   401] iter loss: 1.123\n",
      "[44,   601] iter loss: 1.054\n",
      "Val Accuracy: tensor(63.3200)\n",
      "[45,     1] iter loss: 1.185\n",
      "[45,   201] iter loss: 1.044\n",
      "[45,   401] iter loss: 0.949\n",
      "[45,   601] iter loss: 1.020\n",
      "Val Accuracy: tensor(61.0700)\n",
      "[46,     1] iter loss: 0.891\n",
      "[46,   201] iter loss: 0.969\n",
      "[46,   401] iter loss: 1.139\n",
      "[46,   601] iter loss: 1.066\n",
      "Val Accuracy: tensor(63.4200)\n",
      "[47,     1] iter loss: 0.965\n",
      "[47,   201] iter loss: 0.972\n",
      "[47,   401] iter loss: 0.823\n",
      "[47,   601] iter loss: 0.925\n",
      "Val Accuracy: tensor(63.7000)\n",
      "[48,     1] iter loss: 0.940\n",
      "[48,   201] iter loss: 1.187\n",
      "[48,   401] iter loss: 0.938\n",
      "[48,   601] iter loss: 1.219\n",
      "Val Accuracy: tensor(62.7600)\n",
      "[49,     1] iter loss: 0.880\n",
      "[49,   201] iter loss: 0.985\n",
      "[49,   401] iter loss: 0.777\n",
      "[49,   601] iter loss: 1.085\n",
      "Val Accuracy: tensor(64.1900)\n",
      "[50,     1] iter loss: 0.939\n",
      "[50,   201] iter loss: 1.074\n",
      "[50,   401] iter loss: 0.978\n",
      "[50,   601] iter loss: 0.934\n",
      "Val Accuracy: tensor(63.5200)\n",
      "[51,     1] iter loss: 1.179\n",
      "[51,   201] iter loss: 1.233\n",
      "[51,   401] iter loss: 0.982\n",
      "[51,   601] iter loss: 0.664\n",
      "Val Accuracy: tensor(64.4600)\n",
      "[52,     1] iter loss: 1.152\n",
      "[52,   201] iter loss: 1.092\n",
      "[52,   401] iter loss: 0.881\n",
      "[52,   601] iter loss: 0.895\n",
      "Val Accuracy: tensor(64.4800)\n",
      "[53,     1] iter loss: 0.917\n",
      "[53,   201] iter loss: 0.939\n",
      "[53,   401] iter loss: 0.969\n",
      "[53,   601] iter loss: 0.899\n",
      "Val Accuracy: tensor(63.3100)\n",
      "[54,     1] iter loss: 0.871\n",
      "[54,   201] iter loss: 0.910\n",
      "[54,   401] iter loss: 1.024\n",
      "[54,   601] iter loss: 1.105\n",
      "Val Accuracy: tensor(65.5600)\n",
      "[55,     1] iter loss: 1.283\n",
      "[55,   201] iter loss: 0.945\n",
      "[55,   401] iter loss: 1.068\n",
      "[55,   601] iter loss: 1.102\n",
      "Val Accuracy: tensor(63.7500)\n",
      "[56,     1] iter loss: 0.954\n",
      "[56,   201] iter loss: 1.082\n",
      "[56,   401] iter loss: 1.131\n",
      "[56,   601] iter loss: 1.053\n",
      "Val Accuracy: tensor(62.8500)\n",
      "[57,     1] iter loss: 0.949\n",
      "[57,   201] iter loss: 1.032\n",
      "[57,   401] iter loss: 1.267\n",
      "[57,   601] iter loss: 0.995\n",
      "Val Accuracy: tensor(64.8700)\n",
      "[58,     1] iter loss: 0.832\n",
      "[58,   201] iter loss: 0.780\n",
      "[58,   401] iter loss: 0.886\n",
      "[58,   601] iter loss: 0.908\n",
      "Val Accuracy: tensor(65.3200)\n",
      "[59,     1] iter loss: 0.902\n",
      "[59,   201] iter loss: 0.879\n",
      "[59,   401] iter loss: 0.771\n",
      "[59,   601] iter loss: 0.908\n",
      "Val Accuracy: tensor(64.8900)\n",
      "[60,     1] iter loss: 1.108\n",
      "[60,   201] iter loss: 0.927\n",
      "[60,   401] iter loss: 1.345\n",
      "[60,   601] iter loss: 0.973\n",
      "Val Accuracy: tensor(64.6600)\n",
      "[61,     1] iter loss: 0.917\n",
      "[61,   201] iter loss: 0.711\n",
      "[61,   401] iter loss: 0.814\n",
      "[61,   601] iter loss: 0.830\n",
      "Val Accuracy: tensor(65.6400)\n",
      "[62,     1] iter loss: 0.846\n",
      "[62,   201] iter loss: 0.920\n",
      "[62,   401] iter loss: 0.859\n",
      "[62,   601] iter loss: 0.942\n",
      "Val Accuracy: tensor(65.2400)\n",
      "[63,     1] iter loss: 1.025\n",
      "[63,   201] iter loss: 0.998\n",
      "[63,   401] iter loss: 0.925\n",
      "[63,   601] iter loss: 1.283\n",
      "Val Accuracy: tensor(65.4400)\n",
      "[64,     1] iter loss: 0.867\n",
      "[64,   201] iter loss: 0.765\n",
      "[64,   401] iter loss: 1.074\n",
      "[64,   601] iter loss: 1.088\n",
      "Val Accuracy: tensor(64.9000)\n",
      "[65,     1] iter loss: 0.933\n",
      "[65,   201] iter loss: 0.879\n",
      "[65,   401] iter loss: 1.050\n",
      "[65,   601] iter loss: 1.088\n",
      "Val Accuracy: tensor(66.3400)\n",
      "[66,     1] iter loss: 0.961\n",
      "[66,   201] iter loss: 1.029\n",
      "[66,   401] iter loss: 0.788\n",
      "[66,   601] iter loss: 0.678\n",
      "Val Accuracy: tensor(66.0300)\n",
      "[67,     1] iter loss: 0.810\n",
      "[67,   201] iter loss: 0.777\n",
      "[67,   401] iter loss: 0.940\n",
      "[67,   601] iter loss: 0.843\n",
      "Val Accuracy: tensor(66.0800)\n",
      "[68,     1] iter loss: 0.630\n",
      "[68,   201] iter loss: 1.059\n",
      "[68,   401] iter loss: 0.830\n",
      "[68,   601] iter loss: 0.671\n",
      "Val Accuracy: tensor(67.1000)\n",
      "[69,     1] iter loss: 0.767\n",
      "[69,   201] iter loss: 1.034\n",
      "[69,   401] iter loss: 0.960\n",
      "[69,   601] iter loss: 0.944\n",
      "Val Accuracy: tensor(66.8400)\n",
      "[70,     1] iter loss: 0.772\n",
      "[70,   201] iter loss: 0.849\n",
      "[70,   401] iter loss: 0.724\n",
      "[70,   601] iter loss: 0.885\n",
      "Val Accuracy: tensor(67.4000)\n",
      "[71,     1] iter loss: 0.987\n",
      "[71,   201] iter loss: 0.784\n",
      "[71,   401] iter loss: 0.857\n",
      "[71,   601] iter loss: 1.095\n",
      "Val Accuracy: tensor(67.7800)\n",
      "[72,     1] iter loss: 0.887\n",
      "[72,   201] iter loss: 0.895\n",
      "[72,   401] iter loss: 0.802\n",
      "[72,   601] iter loss: 0.753\n",
      "Val Accuracy: tensor(67.9700)\n",
      "[73,     1] iter loss: 0.842\n",
      "[73,   201] iter loss: 0.716\n",
      "[73,   401] iter loss: 0.831\n",
      "[73,   601] iter loss: 0.955\n",
      "Val Accuracy: tensor(67.5500)\n",
      "[74,     1] iter loss: 0.823\n",
      "[74,   201] iter loss: 0.969\n",
      "[74,   401] iter loss: 0.921\n",
      "[74,   601] iter loss: 0.867\n",
      "Val Accuracy: tensor(66.8200)\n",
      "[75,     1] iter loss: 0.824\n",
      "[75,   201] iter loss: 0.930\n",
      "[75,   401] iter loss: 0.827\n",
      "[75,   601] iter loss: 0.781\n",
      "Val Accuracy: tensor(68.)\n",
      "[76,     1] iter loss: 0.815\n",
      "[76,   201] iter loss: 0.955\n",
      "[76,   401] iter loss: 0.852\n",
      "[76,   601] iter loss: 0.759\n",
      "Val Accuracy: tensor(67.5500)\n",
      "[77,     1] iter loss: 0.924\n",
      "[77,   201] iter loss: 0.728\n",
      "[77,   401] iter loss: 0.896\n",
      "[77,   601] iter loss: 0.761\n",
      "Val Accuracy: tensor(68.2100)\n",
      "[78,     1] iter loss: 0.871\n",
      "[78,   201] iter loss: 0.777\n",
      "[78,   401] iter loss: 0.539\n",
      "[78,   601] iter loss: 0.907\n",
      "Val Accuracy: tensor(67.7500)\n",
      "[79,     1] iter loss: 0.951\n",
      "[79,   201] iter loss: 0.760\n",
      "[79,   401] iter loss: 0.912\n",
      "[79,   601] iter loss: 1.001\n",
      "Val Accuracy: tensor(68.4300)\n",
      "[80,     1] iter loss: 0.676\n",
      "[80,   201] iter loss: 0.805\n",
      "[80,   401] iter loss: 0.710\n",
      "[80,   601] iter loss: 0.727\n",
      "Val Accuracy: tensor(67.6700)\n",
      "[81,     1] iter loss: 0.725\n",
      "[81,   201] iter loss: 0.986\n",
      "[81,   401] iter loss: 0.996\n",
      "[81,   601] iter loss: 0.761\n",
      "Val Accuracy: tensor(68.3600)\n",
      "[82,     1] iter loss: 0.840\n",
      "[82,   201] iter loss: 0.921\n",
      "[82,   401] iter loss: 0.740\n",
      "[82,   601] iter loss: 0.960\n",
      "Val Accuracy: tensor(67.4300)\n",
      "[83,     1] iter loss: 0.812\n",
      "[83,   201] iter loss: 0.919\n",
      "[83,   401] iter loss: 0.751\n",
      "[83,   601] iter loss: 0.811\n",
      "Val Accuracy: tensor(68.0800)\n",
      "[84,     1] iter loss: 0.673\n",
      "[84,   201] iter loss: 0.940\n",
      "[84,   401] iter loss: 0.759\n",
      "[84,   601] iter loss: 0.840\n",
      "Val Accuracy: tensor(69.1400)\n",
      "[85,     1] iter loss: 0.831\n",
      "[85,   201] iter loss: 0.973\n",
      "[85,   401] iter loss: 0.815\n",
      "[85,   601] iter loss: 0.760\n",
      "Val Accuracy: tensor(69.1200)\n",
      "[86,     1] iter loss: 0.561\n",
      "[86,   201] iter loss: 0.628\n",
      "[86,   401] iter loss: 0.680\n",
      "[86,   601] iter loss: 0.939\n",
      "Val Accuracy: tensor(68.4200)\n",
      "[87,     1] iter loss: 1.005\n",
      "[87,   201] iter loss: 0.849\n",
      "[87,   401] iter loss: 0.959\n",
      "[87,   601] iter loss: 1.190\n",
      "Val Accuracy: tensor(69.)\n",
      "[88,     1] iter loss: 0.795\n",
      "[88,   201] iter loss: 0.737\n",
      "[88,   401] iter loss: 0.763\n",
      "[88,   601] iter loss: 0.757\n",
      "Val Accuracy: tensor(69.2400)\n",
      "[89,     1] iter loss: 0.760\n",
      "[89,   201] iter loss: 0.741\n",
      "[89,   401] iter loss: 0.858\n",
      "[89,   601] iter loss: 0.754\n",
      "Val Accuracy: tensor(69.1600)\n",
      "[90,     1] iter loss: 0.709\n",
      "[90,   201] iter loss: 0.908\n",
      "[90,   401] iter loss: 0.893\n",
      "[90,   601] iter loss: 0.932\n",
      "Val Accuracy: tensor(68.8000)\n",
      "[91,     1] iter loss: 0.782\n",
      "[91,   201] iter loss: 0.760\n",
      "[91,   401] iter loss: 0.891\n",
      "[91,   601] iter loss: 0.697\n",
      "Val Accuracy: tensor(68.5900)\n",
      "[92,     1] iter loss: 0.627\n",
      "[92,   201] iter loss: 0.577\n",
      "[92,   401] iter loss: 0.657\n",
      "[92,   601] iter loss: 0.868\n",
      "Val Accuracy: tensor(69.5700)\n",
      "[93,     1] iter loss: 0.907\n",
      "[93,   201] iter loss: 0.734\n",
      "[93,   401] iter loss: 1.008\n",
      "[93,   601] iter loss: 0.716\n",
      "Val Accuracy: tensor(67.6900)\n",
      "[94,     1] iter loss: 0.819\n",
      "[94,   201] iter loss: 0.809\n",
      "[94,   401] iter loss: 0.811\n",
      "[94,   601] iter loss: 0.755\n",
      "Val Accuracy: tensor(69.0700)\n",
      "[95,     1] iter loss: 0.651\n",
      "[95,   201] iter loss: 0.660\n",
      "[95,   401] iter loss: 0.828\n",
      "[95,   601] iter loss: 0.850\n",
      "Val Accuracy: tensor(69.6600)\n",
      "[96,     1] iter loss: 0.601\n",
      "[96,   201] iter loss: 0.813\n",
      "[96,   401] iter loss: 0.716\n",
      "[96,   601] iter loss: 0.788\n",
      "Val Accuracy: tensor(70.0200)\n",
      "[97,     1] iter loss: 0.707\n",
      "[97,   201] iter loss: 0.716\n",
      "[97,   401] iter loss: 0.769\n",
      "[97,   601] iter loss: 0.637\n",
      "Val Accuracy: tensor(70.1700)\n",
      "[98,     1] iter loss: 0.735\n",
      "[98,   201] iter loss: 0.840\n",
      "[98,   401] iter loss: 0.627\n",
      "[98,   601] iter loss: 0.833\n",
      "Val Accuracy: tensor(69.3700)\n",
      "[99,     1] iter loss: 0.812\n",
      "[99,   201] iter loss: 0.860\n",
      "[99,   401] iter loss: 0.755\n",
      "[99,   601] iter loss: 0.766\n",
      "Val Accuracy: tensor(69.3100)\n",
      "[100,     1] iter loss: 0.561\n",
      "[100,   201] iter loss: 0.878\n",
      "[100,   401] iter loss: 0.879\n",
      "[100,   601] iter loss: 0.713\n",
      "Val Accuracy: tensor(69.2700)\n",
      "[101,     1] iter loss: 0.612\n",
      "[101,   201] iter loss: 0.779\n",
      "[101,   401] iter loss: 0.921\n",
      "[101,   601] iter loss: 0.481\n",
      "Val Accuracy: tensor(68.9900)\n",
      "[102,     1] iter loss: 0.557\n",
      "[102,   201] iter loss: 0.754\n",
      "[102,   401] iter loss: 0.735\n",
      "[102,   601] iter loss: 0.617\n",
      "Val Accuracy: tensor(70.4000)\n",
      "[103,     1] iter loss: 0.887\n",
      "[103,   201] iter loss: 0.862\n",
      "[103,   401] iter loss: 0.704\n",
      "[103,   601] iter loss: 0.491\n",
      "Val Accuracy: tensor(70.3300)\n",
      "[104,     1] iter loss: 0.708\n",
      "[104,   201] iter loss: 0.664\n",
      "[104,   401] iter loss: 0.717\n",
      "[104,   601] iter loss: 0.660\n",
      "Val Accuracy: tensor(70.2000)\n",
      "[105,     1] iter loss: 0.646\n",
      "[105,   201] iter loss: 0.723\n",
      "[105,   401] iter loss: 0.778\n",
      "[105,   601] iter loss: 0.806\n",
      "Val Accuracy: tensor(70.2700)\n",
      "[106,     1] iter loss: 0.581\n",
      "[106,   201] iter loss: 0.804\n",
      "[106,   401] iter loss: 0.702\n",
      "[106,   601] iter loss: 0.664\n",
      "Val Accuracy: tensor(70.1500)\n",
      "[107,     1] iter loss: 0.736\n",
      "[107,   201] iter loss: 0.547\n",
      "[107,   401] iter loss: 0.925\n",
      "[107,   601] iter loss: 0.666\n",
      "Val Accuracy: tensor(70.4300)\n",
      "[108,     1] iter loss: 0.556\n",
      "[108,   201] iter loss: 0.827\n",
      "[108,   401] iter loss: 0.691\n",
      "[108,   601] iter loss: 0.588\n",
      "Val Accuracy: tensor(70.3000)\n",
      "[109,     1] iter loss: 0.576\n",
      "[109,   201] iter loss: 0.629\n",
      "[109,   401] iter loss: 1.001\n",
      "[109,   601] iter loss: 0.835\n",
      "Val Accuracy: tensor(70.2700)\n",
      "[110,     1] iter loss: 0.701\n",
      "[110,   201] iter loss: 0.599\n",
      "[110,   401] iter loss: 0.713\n",
      "[110,   601] iter loss: 0.629\n",
      "Val Accuracy: tensor(70.4300)\n",
      "[111,     1] iter loss: 0.655\n",
      "[111,   201] iter loss: 0.660\n",
      "[111,   401] iter loss: 0.643\n",
      "[111,   601] iter loss: 0.606\n",
      "Val Accuracy: tensor(70.4200)\n",
      "[112,     1] iter loss: 0.693\n",
      "[112,   201] iter loss: 0.822\n",
      "[112,   401] iter loss: 0.818\n",
      "[112,   601] iter loss: 0.719\n",
      "Val Accuracy: tensor(70.4600)\n",
      "[113,     1] iter loss: 0.535\n",
      "[113,   201] iter loss: 0.699\n",
      "[113,   401] iter loss: 0.480\n",
      "[113,   601] iter loss: 0.721\n",
      "Val Accuracy: tensor(70.2400)\n",
      "[114,     1] iter loss: 0.683\n",
      "[114,   201] iter loss: 0.621\n",
      "[114,   401] iter loss: 0.679\n",
      "[114,   601] iter loss: 0.741\n",
      "Val Accuracy: tensor(70.8500)\n",
      "[115,     1] iter loss: 0.796\n",
      "[115,   201] iter loss: 0.525\n",
      "[115,   401] iter loss: 0.550\n",
      "[115,   601] iter loss: 0.626\n",
      "Val Accuracy: tensor(70.1800)\n",
      "[116,     1] iter loss: 0.635\n",
      "[116,   201] iter loss: 0.738\n",
      "[116,   401] iter loss: 0.851\n",
      "[116,   601] iter loss: 0.629\n",
      "Val Accuracy: tensor(70.4700)\n",
      "[117,     1] iter loss: 0.799\n",
      "[117,   201] iter loss: 0.779\n",
      "[117,   401] iter loss: 0.638\n",
      "[117,   601] iter loss: 0.522\n",
      "Val Accuracy: tensor(71.0100)\n",
      "[118,     1] iter loss: 0.652\n",
      "[118,   201] iter loss: 0.807\n",
      "[118,   401] iter loss: 0.796\n",
      "[118,   601] iter loss: 0.828\n",
      "Val Accuracy: tensor(70.6100)\n",
      "[119,     1] iter loss: 0.754\n",
      "[119,   201] iter loss: 0.741\n",
      "[119,   401] iter loss: 0.560\n",
      "[119,   601] iter loss: 0.779\n",
      "Val Accuracy: tensor(71.1600)\n",
      "[120,     1] iter loss: 0.502\n",
      "[120,   201] iter loss: 0.657\n",
      "[120,   401] iter loss: 0.794\n",
      "[120,   601] iter loss: 0.467\n",
      "Val Accuracy: tensor(71.1900)\n",
      "[121,     1] iter loss: 0.609\n",
      "[121,   201] iter loss: 0.626\n",
      "[121,   401] iter loss: 0.725\n",
      "[121,   601] iter loss: 0.849\n",
      "Val Accuracy: tensor(71.0200)\n",
      "[122,     1] iter loss: 0.668\n",
      "[122,   201] iter loss: 0.672\n",
      "[122,   401] iter loss: 0.812\n",
      "[122,   601] iter loss: 0.471\n",
      "Val Accuracy: tensor(71.0200)\n",
      "[123,     1] iter loss: 0.541\n",
      "[123,   201] iter loss: 0.845\n",
      "[123,   401] iter loss: 0.834\n",
      "[123,   601] iter loss: 0.841\n",
      "Val Accuracy: tensor(71.9600)\n",
      "[124,     1] iter loss: 0.698\n",
      "[124,   201] iter loss: 0.529\n",
      "[124,   401] iter loss: 0.735\n",
      "[124,   601] iter loss: 0.480\n",
      "Val Accuracy: tensor(71.2200)\n",
      "[125,     1] iter loss: 0.562\n",
      "[125,   201] iter loss: 0.498\n",
      "[125,   401] iter loss: 0.658\n",
      "[125,   601] iter loss: 0.590\n",
      "Val Accuracy: tensor(70.5300)\n",
      "[126,     1] iter loss: 0.671\n",
      "[126,   201] iter loss: 0.705\n",
      "[126,   401] iter loss: 0.631\n",
      "[126,   601] iter loss: 0.683\n",
      "Val Accuracy: tensor(71.7900)\n",
      "[127,     1] iter loss: 0.645\n",
      "[127,   201] iter loss: 0.589\n",
      "[127,   401] iter loss: 0.592\n",
      "[127,   601] iter loss: 0.854\n",
      "Val Accuracy: tensor(71.5400)\n",
      "[128,     1] iter loss: 0.718\n",
      "[128,   201] iter loss: 0.711\n",
      "[128,   401] iter loss: 0.625\n",
      "[128,   601] iter loss: 0.399\n",
      "Val Accuracy: tensor(70.2600)\n",
      "[129,     1] iter loss: 0.572\n",
      "[129,   201] iter loss: 0.488\n",
      "[129,   401] iter loss: 0.622\n",
      "[129,   601] iter loss: 0.536\n",
      "Val Accuracy: tensor(70.6300)\n",
      "[130,     1] iter loss: 0.610\n",
      "[130,   201] iter loss: 0.660\n",
      "[130,   401] iter loss: 0.715\n",
      "[130,   601] iter loss: 0.586\n",
      "Val Accuracy: tensor(71.7400)\n",
      "[131,     1] iter loss: 0.591\n",
      "[131,   201] iter loss: 0.435\n",
      "[131,   401] iter loss: 0.659\n",
      "[131,   601] iter loss: 0.447\n",
      "Val Accuracy: tensor(71.8600)\n",
      "[132,     1] iter loss: 0.527\n",
      "[132,   201] iter loss: 0.606\n",
      "[132,   401] iter loss: 0.666\n",
      "[132,   601] iter loss: 0.388\n",
      "Val Accuracy: tensor(71.0800)\n",
      "[133,     1] iter loss: 0.647\n",
      "[133,   201] iter loss: 0.608\n",
      "[133,   401] iter loss: 0.640\n",
      "[133,   601] iter loss: 0.578\n",
      "Val Accuracy: tensor(71.1400)\n",
      "[134,     1] iter loss: 0.668\n",
      "[134,   201] iter loss: 0.476\n",
      "[134,   401] iter loss: 0.585\n",
      "[134,   601] iter loss: 0.612\n",
      "Val Accuracy: tensor(70.8500)\n",
      "[135,     1] iter loss: 0.510\n",
      "[135,   201] iter loss: 0.611\n",
      "[135,   401] iter loss: 0.613\n",
      "[135,   601] iter loss: 0.420\n",
      "Val Accuracy: tensor(71.4200)\n",
      "[136,     1] iter loss: 0.596\n",
      "[136,   201] iter loss: 0.703\n",
      "[136,   401] iter loss: 0.593\n",
      "[136,   601] iter loss: 0.696\n",
      "Val Accuracy: tensor(70.7600)\n",
      "[137,     1] iter loss: 0.602\n",
      "[137,   201] iter loss: 0.729\n",
      "[137,   401] iter loss: 0.625\n",
      "[137,   601] iter loss: 0.676\n",
      "Val Accuracy: tensor(72.)\n",
      "[138,     1] iter loss: 0.709\n",
      "[138,   201] iter loss: 0.804\n",
      "[138,   401] iter loss: 0.445\n",
      "[138,   601] iter loss: 0.592\n",
      "Val Accuracy: tensor(71.0200)\n",
      "[139,     1] iter loss: 0.561\n",
      "[139,   201] iter loss: 0.552\n",
      "[139,   401] iter loss: 0.479\n",
      "[139,   601] iter loss: 0.603\n",
      "Val Accuracy: tensor(71.3400)\n",
      "[140,     1] iter loss: 0.513\n",
      "[140,   201] iter loss: 0.660\n",
      "[140,   401] iter loss: 0.815\n",
      "[140,   601] iter loss: 0.449\n",
      "Val Accuracy: tensor(71.5000)\n",
      "[141,     1] iter loss: 0.485\n",
      "[141,   201] iter loss: 0.574\n",
      "[141,   401] iter loss: 0.392\n",
      "[141,   601] iter loss: 0.562\n",
      "Val Accuracy: tensor(71.6700)\n",
      "[142,     1] iter loss: 0.585\n",
      "[142,   201] iter loss: 0.476\n",
      "[142,   401] iter loss: 0.774\n",
      "[142,   601] iter loss: 0.540\n",
      "Val Accuracy: tensor(71.6100)\n",
      "[143,     1] iter loss: 0.447\n",
      "[143,   201] iter loss: 0.498\n",
      "[143,   401] iter loss: 0.647\n",
      "[143,   601] iter loss: 0.471\n",
      "Val Accuracy: tensor(71.9200)\n",
      "[144,     1] iter loss: 0.507\n",
      "[144,   201] iter loss: 0.707\n",
      "[144,   401] iter loss: 0.657\n",
      "[144,   601] iter loss: 0.554\n",
      "Val Accuracy: tensor(71.8500)\n",
      "[145,     1] iter loss: 0.405\n",
      "[145,   201] iter loss: 0.606\n",
      "[145,   401] iter loss: 0.533\n",
      "[145,   601] iter loss: 0.635\n",
      "Val Accuracy: tensor(71.5500)\n",
      "[146,     1] iter loss: 0.483\n",
      "[146,   201] iter loss: 0.575\n",
      "[146,   401] iter loss: 0.760\n",
      "[146,   601] iter loss: 0.631\n",
      "Val Accuracy: tensor(72.6600)\n",
      "[147,     1] iter loss: 0.496\n",
      "[147,   201] iter loss: 0.570\n",
      "[147,   401] iter loss: 0.451\n",
      "[147,   601] iter loss: 0.585\n",
      "Val Accuracy: tensor(71.4200)\n",
      "[148,     1] iter loss: 0.423\n",
      "[148,   201] iter loss: 0.408\n",
      "[148,   401] iter loss: 0.564\n",
      "[148,   601] iter loss: 0.643\n",
      "Val Accuracy: tensor(71.3600)\n",
      "[149,     1] iter loss: 0.454\n",
      "[149,   201] iter loss: 0.591\n",
      "[149,   401] iter loss: 0.502\n",
      "[149,   601] iter loss: 0.591\n",
      "Val Accuracy: tensor(71.8400)\n",
      "[150,     1] iter loss: 0.523\n",
      "[150,   201] iter loss: 0.598\n",
      "[150,   401] iter loss: 0.640\n",
      "[150,   601] iter loss: 0.554\n",
      "Val Accuracy: tensor(72.0600)\n",
      "[151,     1] iter loss: 0.521\n",
      "[151,   201] iter loss: 0.630\n",
      "[151,   401] iter loss: 0.476\n",
      "[151,   601] iter loss: 0.493\n",
      "Val Accuracy: tensor(72.0900)\n",
      "[152,     1] iter loss: 0.527\n",
      "[152,   201] iter loss: 0.568\n",
      "[152,   401] iter loss: 0.595\n",
      "[152,   601] iter loss: 0.675\n",
      "Val Accuracy: tensor(71.1900)\n",
      "[153,     1] iter loss: 0.410\n",
      "[153,   201] iter loss: 0.619\n",
      "[153,   401] iter loss: 0.474\n",
      "[153,   601] iter loss: 0.460\n",
      "Val Accuracy: tensor(72.3000)\n",
      "[154,     1] iter loss: 0.551\n",
      "[154,   201] iter loss: 0.455\n",
      "[154,   401] iter loss: 0.461\n",
      "[154,   601] iter loss: 0.692\n",
      "Val Accuracy: tensor(72.0400)\n",
      "[155,     1] iter loss: 0.616\n",
      "[155,   201] iter loss: 0.569\n",
      "[155,   401] iter loss: 0.424\n",
      "[155,   601] iter loss: 0.618\n",
      "Val Accuracy: tensor(71.6400)\n",
      "[156,     1] iter loss: 0.549\n",
      "[156,   201] iter loss: 0.451\n",
      "[156,   401] iter loss: 0.521\n",
      "[156,   601] iter loss: 0.496\n",
      "Val Accuracy: tensor(72.0500)\n",
      "[157,     1] iter loss: 0.658\n",
      "[157,   201] iter loss: 0.680\n",
      "[157,   401] iter loss: 0.622\n",
      "[157,   601] iter loss: 0.418\n",
      "Val Accuracy: tensor(71.3200)\n",
      "[158,     1] iter loss: 0.505\n",
      "[158,   201] iter loss: 0.573\n",
      "[158,   401] iter loss: 0.477\n",
      "[158,   601] iter loss: 0.514\n",
      "Val Accuracy: tensor(71.6700)\n",
      "[159,     1] iter loss: 0.445\n",
      "[159,   201] iter loss: 0.439\n",
      "[159,   401] iter loss: 0.433\n",
      "[159,   601] iter loss: 0.645\n",
      "Val Accuracy: tensor(71.8700)\n",
      "[160,     1] iter loss: 0.369\n",
      "[160,   201] iter loss: 0.572\n",
      "[160,   401] iter loss: 0.486\n",
      "[160,   601] iter loss: 0.587\n",
      "Val Accuracy: tensor(71.8700)\n",
      "[161,     1] iter loss: 0.465\n",
      "[161,   201] iter loss: 0.439\n",
      "[161,   401] iter loss: 0.549\n",
      "[161,   601] iter loss: 0.600\n",
      "Val Accuracy: tensor(71.8500)\n",
      "[162,     1] iter loss: 0.521\n",
      "[162,   201] iter loss: 0.523\n",
      "[162,   401] iter loss: 0.522\n",
      "[162,   601] iter loss: 0.467\n",
      "Val Accuracy: tensor(72.2800)\n",
      "[163,     1] iter loss: 0.428\n",
      "[163,   201] iter loss: 0.434\n",
      "[163,   401] iter loss: 0.513\n",
      "[163,   601] iter loss: 0.428\n",
      "Val Accuracy: tensor(71.6700)\n",
      "[164,     1] iter loss: 0.440\n",
      "[164,   201] iter loss: 0.496\n",
      "[164,   401] iter loss: 0.636\n",
      "[164,   601] iter loss: 0.392\n",
      "Val Accuracy: tensor(71.9700)\n",
      "[165,     1] iter loss: 0.474\n",
      "[165,   201] iter loss: 0.735\n",
      "[165,   401] iter loss: 0.521\n",
      "[165,   601] iter loss: 0.587\n",
      "Val Accuracy: tensor(72.5100)\n",
      "[166,     1] iter loss: 0.463\n",
      "[166,   201] iter loss: 0.786\n",
      "[166,   401] iter loss: 0.579\n",
      "[166,   601] iter loss: 0.666\n",
      "Val Accuracy: tensor(72.8400)\n",
      "[167,     1] iter loss: 0.377\n",
      "[167,   201] iter loss: 0.392\n",
      "[167,   401] iter loss: 0.415\n",
      "[167,   601] iter loss: 0.363\n",
      "Val Accuracy: tensor(71.6000)\n",
      "[168,     1] iter loss: 0.358\n",
      "[168,   201] iter loss: 0.626\n",
      "[168,   401] iter loss: 0.425\n",
      "[168,   601] iter loss: 0.451\n",
      "Val Accuracy: tensor(72.4200)\n",
      "[169,     1] iter loss: 0.433\n",
      "[169,   201] iter loss: 0.457\n",
      "[169,   401] iter loss: 0.436\n",
      "[169,   601] iter loss: 0.505\n",
      "Val Accuracy: tensor(71.6800)\n",
      "[170,     1] iter loss: 0.454\n",
      "[170,   201] iter loss: 0.616\n",
      "[170,   401] iter loss: 0.492\n",
      "[170,   601] iter loss: 0.547\n",
      "Val Accuracy: tensor(72.4200)\n",
      "[171,     1] iter loss: 0.414\n",
      "[171,   201] iter loss: 0.563\n",
      "[171,   401] iter loss: 0.422\n",
      "[171,   601] iter loss: 0.414\n",
      "Val Accuracy: tensor(71.6500)\n",
      "[172,     1] iter loss: 0.382\n",
      "[172,   201] iter loss: 0.482\n",
      "[172,   401] iter loss: 0.493\n",
      "[172,   601] iter loss: 0.424\n",
      "Val Accuracy: tensor(72.6200)\n",
      "[173,     1] iter loss: 0.360\n",
      "[173,   201] iter loss: 0.452\n",
      "[173,   401] iter loss: 0.533\n",
      "[173,   601] iter loss: 0.379\n",
      "Val Accuracy: tensor(72.4600)\n",
      "[174,     1] iter loss: 0.425\n",
      "[174,   201] iter loss: 0.447\n",
      "[174,   401] iter loss: 0.543\n",
      "[174,   601] iter loss: 0.584\n",
      "Val Accuracy: tensor(71.0400)\n",
      "[175,     1] iter loss: 0.400\n",
      "[175,   201] iter loss: 0.571\n",
      "[175,   401] iter loss: 0.398\n",
      "[175,   601] iter loss: 0.255\n",
      "Val Accuracy: tensor(72.6900)\n",
      "[176,     1] iter loss: 0.470\n",
      "[176,   201] iter loss: 0.480\n",
      "[176,   401] iter loss: 0.490\n",
      "[176,   601] iter loss: 0.399\n",
      "Val Accuracy: tensor(72.2000)\n",
      "[177,     1] iter loss: 0.383\n",
      "[177,   201] iter loss: 0.324\n",
      "[177,   401] iter loss: 0.272\n",
      "[177,   601] iter loss: 0.591\n",
      "Val Accuracy: tensor(72.1000)\n",
      "[178,     1] iter loss: 0.399\n",
      "[178,   201] iter loss: 0.465\n",
      "[178,   401] iter loss: 0.700\n",
      "[178,   601] iter loss: 0.496\n",
      "Val Accuracy: tensor(71.8700)\n",
      "[179,     1] iter loss: 0.526\n",
      "[179,   201] iter loss: 0.349\n",
      "[179,   401] iter loss: 0.476\n",
      "[179,   601] iter loss: 0.420\n",
      "Val Accuracy: tensor(71.7600)\n",
      "[180,     1] iter loss: 0.462\n",
      "[180,   201] iter loss: 0.422\n",
      "[180,   401] iter loss: 0.361\n",
      "[180,   601] iter loss: 0.355\n",
      "Val Accuracy: tensor(72.4400)\n",
      "[181,     1] iter loss: 0.365\n",
      "[181,   201] iter loss: 0.334\n",
      "[181,   401] iter loss: 0.614\n",
      "[181,   601] iter loss: 0.378\n",
      "Val Accuracy: tensor(72.4500)\n",
      "[182,     1] iter loss: 0.474\n",
      "[182,   201] iter loss: 0.533\n",
      "[182,   401] iter loss: 0.495\n",
      "[182,   601] iter loss: 0.342\n",
      "Val Accuracy: tensor(72.5700)\n",
      "[183,     1] iter loss: 0.399\n",
      "[183,   201] iter loss: 0.363\n",
      "[183,   401] iter loss: 0.579\n",
      "[183,   601] iter loss: 0.404\n",
      "Val Accuracy: tensor(72.3300)\n",
      "[184,     1] iter loss: 0.405\n",
      "[184,   201] iter loss: 0.717\n",
      "[184,   401] iter loss: 0.351\n",
      "[184,   601] iter loss: 0.458\n",
      "Val Accuracy: tensor(72.8100)\n",
      "[185,     1] iter loss: 0.357\n",
      "[185,   201] iter loss: 0.407\n",
      "[185,   401] iter loss: 0.391\n",
      "[185,   601] iter loss: 0.484\n",
      "Val Accuracy: tensor(72.7300)\n",
      "[186,     1] iter loss: 0.284\n",
      "[186,   201] iter loss: 0.298\n",
      "[186,   401] iter loss: 0.269\n",
      "[186,   601] iter loss: 0.385\n",
      "Val Accuracy: tensor(72.4600)\n",
      "[187,     1] iter loss: 0.600\n",
      "[187,   201] iter loss: 0.530\n",
      "[187,   401] iter loss: 0.395\n",
      "[187,   601] iter loss: 0.420\n",
      "Val Accuracy: tensor(71.9500)\n",
      "[188,     1] iter loss: 0.315\n",
      "[188,   201] iter loss: 0.225\n",
      "[188,   401] iter loss: 0.378\n",
      "[188,   601] iter loss: 0.416\n",
      "Val Accuracy: tensor(71.9400)\n",
      "[189,     1] iter loss: 0.463\n",
      "[189,   201] iter loss: 0.419\n",
      "[189,   401] iter loss: 0.499\n",
      "[189,   601] iter loss: 0.271\n",
      "Val Accuracy: tensor(72.7600)\n",
      "[190,     1] iter loss: 0.363\n",
      "[190,   201] iter loss: 0.685\n",
      "[190,   401] iter loss: 0.490\n",
      "[190,   601] iter loss: 0.583\n",
      "Val Accuracy: tensor(72.3600)\n",
      "[191,     1] iter loss: 0.498\n",
      "[191,   201] iter loss: 0.418\n",
      "[191,   401] iter loss: 0.547\n",
      "[191,   601] iter loss: 0.388\n",
      "Val Accuracy: tensor(71.8800)\n",
      "[192,     1] iter loss: 0.372\n",
      "[192,   201] iter loss: 0.431\n",
      "[192,   401] iter loss: 0.331\n",
      "[192,   601] iter loss: 0.467\n",
      "Val Accuracy: tensor(72.9800)\n",
      "[193,     1] iter loss: 0.358\n",
      "[193,   201] iter loss: 0.246\n",
      "[193,   401] iter loss: 0.409\n",
      "[193,   601] iter loss: 0.406\n",
      "Val Accuracy: tensor(72.2200)\n",
      "[194,     1] iter loss: 0.465\n",
      "[194,   201] iter loss: 0.426\n",
      "[194,   401] iter loss: 0.428\n",
      "[194,   601] iter loss: 0.318\n",
      "Val Accuracy: tensor(72.0600)\n",
      "[195,     1] iter loss: 0.339\n",
      "[195,   201] iter loss: 0.381\n",
      "[195,   401] iter loss: 0.332\n",
      "[195,   601] iter loss: 0.336\n",
      "Val Accuracy: tensor(72.2900)\n",
      "[196,     1] iter loss: 0.340\n",
      "[196,   201] iter loss: 0.325\n",
      "[196,   401] iter loss: 0.338\n",
      "[196,   601] iter loss: 0.432\n",
      "Val Accuracy: tensor(72.2600)\n",
      "[197,     1] iter loss: 0.424\n",
      "[197,   201] iter loss: 0.546\n",
      "[197,   401] iter loss: 0.416\n",
      "[197,   601] iter loss: 0.334\n",
      "Val Accuracy: tensor(71.9700)\n",
      "[198,     1] iter loss: 0.372\n",
      "[198,   201] iter loss: 0.350\n",
      "[198,   401] iter loss: 0.316\n",
      "[198,   601] iter loss: 0.450\n",
      "Val Accuracy: tensor(72.2500)\n",
      "[199,     1] iter loss: 0.364\n",
      "[199,   201] iter loss: 0.232\n",
      "[199,   401] iter loss: 0.315\n",
      "[199,   601] iter loss: 0.319\n",
      "Val Accuracy: tensor(72.5200)\n",
      "[200,     1] iter loss: 0.339\n",
      "[200,   201] iter loss: 0.196\n",
      "[200,   401] iter loss: 0.461\n",
      "[200,   601] iter loss: 0.553\n",
      "Val Accuracy: tensor(72.3900)\n",
      "[201,     1] iter loss: 0.435\n",
      "[201,   201] iter loss: 0.270\n",
      "[201,   401] iter loss: 0.314\n",
      "[201,   601] iter loss: 0.264\n",
      "Val Accuracy: tensor(72.2200)\n",
      "[202,     1] iter loss: 0.391\n",
      "[202,   201] iter loss: 0.369\n",
      "[202,   401] iter loss: 0.358\n",
      "[202,   601] iter loss: 0.636\n",
      "Val Accuracy: tensor(72.6500)\n",
      "[203,     1] iter loss: 0.424\n",
      "[203,   201] iter loss: 0.329\n",
      "[203,   401] iter loss: 0.535\n",
      "[203,   601] iter loss: 0.654\n",
      "Val Accuracy: tensor(71.8000)\n",
      "[204,     1] iter loss: 0.460\n",
      "[204,   201] iter loss: 0.283\n",
      "[204,   401] iter loss: 0.380\n",
      "[204,   601] iter loss: 0.527\n",
      "Val Accuracy: tensor(72.5900)\n",
      "[205,     1] iter loss: 0.428\n",
      "[205,   201] iter loss: 0.268\n",
      "[205,   401] iter loss: 0.308\n",
      "[205,   601] iter loss: 0.465\n",
      "Val Accuracy: tensor(71.6600)\n",
      "[206,     1] iter loss: 0.360\n",
      "[206,   201] iter loss: 0.541\n",
      "[206,   401] iter loss: 0.548\n",
      "[206,   601] iter loss: 0.572\n",
      "Val Accuracy: tensor(72.8000)\n",
      "[207,     1] iter loss: 0.244\n",
      "[207,   201] iter loss: 0.317\n",
      "[207,   401] iter loss: 0.353\n",
      "[207,   601] iter loss: 0.378\n",
      "Val Accuracy: tensor(71.9700)\n",
      "[208,     1] iter loss: 0.355\n",
      "[208,   201] iter loss: 0.232\n",
      "[208,   401] iter loss: 0.509\n",
      "[208,   601] iter loss: 0.466\n",
      "Val Accuracy: tensor(72.1200)\n",
      "[209,     1] iter loss: 0.323\n",
      "[209,   201] iter loss: 0.461\n",
      "[209,   401] iter loss: 0.312\n",
      "[209,   601] iter loss: 0.494\n",
      "Val Accuracy: tensor(72.9900)\n",
      "[210,     1] iter loss: 0.285\n",
      "[210,   201] iter loss: 0.369\n",
      "[210,   401] iter loss: 0.257\n",
      "[210,   601] iter loss: 0.330\n",
      "Val Accuracy: tensor(72.7800)\n",
      "[211,     1] iter loss: 0.334\n",
      "[211,   201] iter loss: 0.237\n",
      "[211,   401] iter loss: 0.583\n",
      "[211,   601] iter loss: 0.364\n",
      "Val Accuracy: tensor(72.8100)\n",
      "[212,     1] iter loss: 0.285\n",
      "[212,   201] iter loss: 0.252\n",
      "[212,   401] iter loss: 0.419\n",
      "[212,   601] iter loss: 0.287\n",
      "Val Accuracy: tensor(73.0700)\n",
      "[213,     1] iter loss: 0.264\n",
      "[213,   201] iter loss: 0.411\n",
      "[213,   401] iter loss: 0.443\n",
      "[213,   601] iter loss: 0.309\n",
      "Val Accuracy: tensor(72.7300)\n",
      "[214,     1] iter loss: 0.202\n",
      "[214,   201] iter loss: 0.297\n",
      "[214,   401] iter loss: 0.294\n",
      "[214,   601] iter loss: 0.351\n",
      "Val Accuracy: tensor(72.4500)\n",
      "[215,     1] iter loss: 0.324\n",
      "[215,   201] iter loss: 0.453\n",
      "[215,   401] iter loss: 0.350\n",
      "[215,   601] iter loss: 0.228\n",
      "Val Accuracy: tensor(72.7300)\n",
      "[216,     1] iter loss: 0.294\n",
      "[216,   201] iter loss: 0.321\n",
      "[216,   401] iter loss: 0.356\n",
      "[216,   601] iter loss: 0.456\n",
      "Val Accuracy: tensor(72.3800)\n",
      "[217,     1] iter loss: 0.404\n",
      "[217,   201] iter loss: 0.279\n",
      "[217,   401] iter loss: 0.444\n",
      "[217,   601] iter loss: 0.395\n",
      "Val Accuracy: tensor(72.1800)\n",
      "[218,     1] iter loss: 0.346\n",
      "[218,   201] iter loss: 0.469\n",
      "[218,   401] iter loss: 0.296\n",
      "[218,   601] iter loss: 0.270\n",
      "Val Accuracy: tensor(72.2900)\n",
      "[219,     1] iter loss: 0.164\n",
      "[219,   201] iter loss: 0.364\n",
      "[219,   401] iter loss: 0.340\n",
      "[219,   601] iter loss: 0.269\n",
      "Val Accuracy: tensor(72.6400)\n",
      "[220,     1] iter loss: 0.271\n",
      "[220,   201] iter loss: 0.380\n",
      "[220,   401] iter loss: 0.229\n",
      "[220,   601] iter loss: 0.306\n",
      "Val Accuracy: tensor(71.8700)\n",
      "[221,     1] iter loss: 0.275\n",
      "[221,   201] iter loss: 0.524\n",
      "[221,   401] iter loss: 0.388\n",
      "[221,   601] iter loss: 0.379\n",
      "Val Accuracy: tensor(72.1000)\n",
      "[222,     1] iter loss: 0.334\n",
      "[222,   201] iter loss: 0.207\n",
      "[222,   401] iter loss: 0.433\n",
      "[222,   601] iter loss: 0.249\n",
      "Val Accuracy: tensor(71.1900)\n",
      "[223,     1] iter loss: 0.287\n",
      "[223,   201] iter loss: 0.236\n",
      "[223,   401] iter loss: 0.359\n",
      "[223,   601] iter loss: 0.351\n",
      "Val Accuracy: tensor(72.3600)\n",
      "[224,     1] iter loss: 0.180\n",
      "[224,   201] iter loss: 0.386\n",
      "[224,   401] iter loss: 0.323\n",
      "[224,   601] iter loss: 0.395\n",
      "Val Accuracy: tensor(72.4900)\n",
      "[225,     1] iter loss: 0.238\n",
      "[225,   201] iter loss: 0.428\n",
      "[225,   401] iter loss: 0.434\n",
      "[225,   601] iter loss: 0.436\n",
      "Val Accuracy: tensor(71.8700)\n",
      "[226,     1] iter loss: 0.236\n",
      "[226,   201] iter loss: 0.427\n",
      "[226,   401] iter loss: 0.246\n",
      "[226,   601] iter loss: 0.177\n",
      "Val Accuracy: tensor(72.9800)\n",
      "[227,     1] iter loss: 0.304\n",
      "[227,   201] iter loss: 0.260\n",
      "[227,   401] iter loss: 0.301\n",
      "[227,   601] iter loss: 0.327\n",
      "Val Accuracy: tensor(72.4300)\n",
      "[228,     1] iter loss: 0.200\n",
      "[228,   201] iter loss: 0.387\n",
      "[228,   401] iter loss: 0.387\n",
      "[228,   601] iter loss: 0.247\n",
      "Val Accuracy: tensor(72.7200)\n",
      "[229,     1] iter loss: 0.253\n",
      "[229,   201] iter loss: 0.249\n",
      "[229,   401] iter loss: 0.229\n",
      "[229,   601] iter loss: 0.397\n",
      "Val Accuracy: tensor(73.)\n",
      "[230,     1] iter loss: 0.456\n",
      "[230,   201] iter loss: 0.583\n",
      "[230,   401] iter loss: 0.388\n",
      "[230,   601] iter loss: 0.249\n",
      "Val Accuracy: tensor(72.5600)\n",
      "[231,     1] iter loss: 0.221\n",
      "[231,   201] iter loss: 0.339\n",
      "[231,   401] iter loss: 0.319\n",
      "[231,   601] iter loss: 0.438\n",
      "Val Accuracy: tensor(72.1700)\n",
      "[232,     1] iter loss: 0.346\n",
      "[232,   201] iter loss: 0.289\n",
      "[232,   401] iter loss: 0.377\n",
      "[232,   601] iter loss: 0.375\n",
      "Val Accuracy: tensor(73.2000)\n",
      "[233,     1] iter loss: 0.209\n",
      "[233,   201] iter loss: 0.375\n",
      "[233,   401] iter loss: 0.275\n",
      "[233,   601] iter loss: 0.166\n",
      "Val Accuracy: tensor(72.2800)\n",
      "[234,     1] iter loss: 0.172\n",
      "[234,   201] iter loss: 0.277\n",
      "[234,   401] iter loss: 0.291\n",
      "[234,   601] iter loss: 0.221\n",
      "Val Accuracy: tensor(72.0200)\n",
      "[235,     1] iter loss: 0.540\n",
      "[235,   201] iter loss: 0.258\n",
      "[235,   401] iter loss: 0.415\n",
      "[235,   601] iter loss: 0.505\n",
      "Val Accuracy: tensor(72.7300)\n",
      "[236,     1] iter loss: 0.258\n",
      "[236,   201] iter loss: 0.393\n",
      "[236,   401] iter loss: 0.238\n",
      "[236,   601] iter loss: 0.241\n",
      "Val Accuracy: tensor(72.3500)\n",
      "[237,     1] iter loss: 0.247\n",
      "[237,   201] iter loss: 0.293\n",
      "[237,   401] iter loss: 0.374\n",
      "[237,   601] iter loss: 0.256\n",
      "Val Accuracy: tensor(72.7100)\n",
      "[238,     1] iter loss: 0.339\n",
      "[238,   201] iter loss: 0.259\n",
      "[238,   401] iter loss: 0.335\n",
      "[238,   601] iter loss: 0.318\n",
      "Val Accuracy: tensor(72.4400)\n",
      "[239,     1] iter loss: 0.323\n",
      "[239,   201] iter loss: 0.268\n",
      "[239,   401] iter loss: 0.301\n",
      "[239,   601] iter loss: 0.424\n",
      "Val Accuracy: tensor(73.3500)\n",
      "[240,     1] iter loss: 0.178\n",
      "[240,   201] iter loss: 0.261\n",
      "[240,   401] iter loss: 0.312\n",
      "[240,   601] iter loss: 0.250\n",
      "Val Accuracy: tensor(72.2200)\n",
      "[241,     1] iter loss: 0.277\n",
      "[241,   201] iter loss: 0.253\n",
      "[241,   401] iter loss: 0.182\n",
      "[241,   601] iter loss: 0.141\n",
      "Val Accuracy: tensor(71.9100)\n",
      "[242,     1] iter loss: 0.296\n",
      "[242,   201] iter loss: 0.312\n",
      "[242,   401] iter loss: 0.198\n",
      "[242,   601] iter loss: 0.222\n",
      "Val Accuracy: tensor(72.5900)\n",
      "[243,     1] iter loss: 0.477\n",
      "[243,   201] iter loss: 0.300\n",
      "[243,   401] iter loss: 0.339\n",
      "[243,   601] iter loss: 0.275\n",
      "Val Accuracy: tensor(71.5600)\n",
      "[244,     1] iter loss: 0.179\n",
      "[244,   201] iter loss: 0.246\n",
      "[244,   401] iter loss: 0.112\n",
      "[244,   601] iter loss: 0.157\n",
      "Val Accuracy: tensor(72.4500)\n",
      "[245,     1] iter loss: 0.330\n",
      "[245,   201] iter loss: 0.267\n",
      "[245,   401] iter loss: 0.170\n",
      "[245,   601] iter loss: 0.384\n",
      "Val Accuracy: tensor(72.9800)\n",
      "[246,     1] iter loss: 0.260\n",
      "[246,   201] iter loss: 0.260\n",
      "[246,   401] iter loss: 0.402\n",
      "[246,   601] iter loss: 0.250\n",
      "Val Accuracy: tensor(73.1500)\n",
      "[247,     1] iter loss: 0.147\n",
      "[247,   201] iter loss: 0.368\n",
      "[247,   401] iter loss: 0.278\n",
      "[247,   601] iter loss: 0.139\n",
      "Val Accuracy: tensor(72.9800)\n",
      "[248,     1] iter loss: 0.343\n",
      "[248,   201] iter loss: 0.307\n",
      "[248,   401] iter loss: 0.167\n",
      "[248,   601] iter loss: 0.403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-482c36085cbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mpytorch_total_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total trainable parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_total_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3b017c989705>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, epochs, accumulation_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0mclipping_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# arbitrary value of your choosing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the running script cell. Make relevant folders  in your google drive before running this!\n",
    "\"\"\"\n",
    "trainloader,val, test=get_loaders(batch_size=128,num_workers=10)\n",
    "stages=['base_model', 'exp_expansion', 'linear_expansion', 'reload']\n",
    "path=F\"/content/gdrive/My Drive/CSNModels/experiments/CSN_CNN_withpruning\"\n",
    "torch.cuda.empty_cache()\n",
    "stages=['base_model', 'exp_expansion', 'linear_expansion', 'reload']\n",
    "\n",
    "\n",
    "model=CompositionalSparseNetCNN(exp_level=1, exponential_cutoff=1,expo_depth=2,expo_width=2,stage=stages[0],\n",
    "                            prev_path=path, channels_width=128).cuda()\n",
    "train(model,trainloader,val,epochs=200)\n",
    "print(\"Training Accuracy: \", validation(model,trainloader))\n",
    "print(\"Val Accuracy:\",validation(model,val))\n",
    "print(\"Test Accuracy:\",validation(model,test))\n",
    "ranmodel=CompositionalSparseNetCNN(exp_level=1, exponential_cutoff=1,expo_depth=2,expo_width=2,stage=stages[0],\n",
    "                            prev_path=path, channels_width=128).cuda()\n",
    "model=randomize_pruning(model,ranmodel)\n",
    "torch.save(model.state_dict(),path+\"/level1.pth\")\n",
    "del model\n",
    "del ranmodel\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model=CompositionalSparseNetCNN(exp_level=2, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[1],\n",
    "                            prev_path=path, channels_width=256).cuda()\n",
    "structured_pruning(model, pruning_rate=0.5) # it will also remove interconnections , but that's okay actually. More stochasticity. \n",
    "train(model,trainloader,val,epochs=500)\n",
    "print(\"Training Accuracy\", validation(model,trainloader))\n",
    "print(\"Val Accuracy:\",validation(model,val))\n",
    "print(\"Test Accuracy:\",validation(model,test))\n",
    "remove_structured_pruning(model)\n",
    "ranmodel=CompositionalSparseNetCNN(exp_level=2, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[0],\n",
    "                            prev_path=path, channels_width=256).cuda()\n",
    "model=randomize_pruning(model,ranmodel)\n",
    "torch.save(model.state_dict(),path+\"/level2.pth\")\n",
    "\n",
    "del model\n",
    "del ranmodel\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model=CompositionalSparseNetCNN(exp_level=3, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[2],channels_width=256,\n",
    "                            prev_path=path).cuda()\n",
    "structured_pruning(model, pruning_rate=0.5) # it will also remove interconnections .\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters:\", pytorch_total_params)\n",
    "train(model,trainloader,val,epochs=500)\n",
    "print(\"Training Accuracy\", validation(model,trainloader))\n",
    "print(\"Val Accuracy:\",validation(model,val))\n",
    "print(\"Test Accuracy:\",validation(model,test))\n",
    "remove_structured_pruning(model)\n",
    "ranmodel=CompositionalSparseNetCNN(exp_level=3, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[0],channels_width=256,\n",
    "                            prev_path=path).cuda()\n",
    "model=randomize_pruning(model,ranmodel)\n",
    "torch.save(model.state_dict(),path+\"/level3.pth\")\n",
    "\n",
    "del model\n",
    "del ranmodel\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainloader,val, test=get_loaders(batch_size=64,num_workers=5)\n",
    "model=CompositionalSparseNetCNN(exp_level=4, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[2],channels_width=256,\n",
    "                            prev_path=path).cuda()\n",
    "structured_pruning(model, pruning_rate=0.5) # it will also remove interconnections.\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters:\", pytorch_total_params)\n",
    "train(model,trainloader,val,epochs=500)\n",
    "print(\"Training Accuracy\", validation(model,trainloader))\n",
    "print(\"Val Accuracy:\",validation(model,val))\n",
    "print(\"Test Accuracy:\",validation(model,test))\n",
    "remove_structured_pruning(model)\n",
    "# ranmodel=CompositionalSparseNetCNN(exp_level=4, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[0],channels_width=256,\n",
    "#                             prev_path=path).cuda()\n",
    "# model=randomize_pruning(model,ranmodel)\n",
    "torch.save(model.state_dict(),path+\"/level4.pth\")\n",
    "\n",
    "# model=CompositionalSparseNetCNN(exp_level=5, exponential_cutoff=2,expo_depth=2,expo_width=2,stage=stages[2],channels_width=256,\n",
    "#                             prev_path=path).cuda()\n",
    "# # structured_pruning(model, pruning_rate=0.3) # it will also remove interconnections dammit.\n",
    "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"Total trainable parameters:\", pytorch_total_params)\n",
    "# train(model,trainloader,val,epochs=500)\n",
    "# print(\"Training Accuracy\", validation(model,trainloader))\n",
    "# print(\"Val Accuracy:\",validation(model,val))\n",
    "# print(\"Test Accuracy:\",validation(model,test))\n",
    "# remove_structured_pruning(model)\n",
    "# ranmodel=CompositionalSparseNetCNN(exp_level=4, exponential_cutoff=1,expo_depth=2,expo_width=2,stage=stages[0],channels_width=128,\n",
    "#                             prev_path=path).cuda()\n",
    "# model=randomize_pruning(model,ranmodel)\n",
    "# torch.save(model.state_dict(),path+\"/level5.pth\")\n",
    "# del model\n",
    "# del ranmodel\n",
    "# torch.cuda.empty_cache()\n",
    "# trainloader, val, test=get_loaders(batch_size=32,num_workers=8)\n",
    "# model=CompositionalSparseNetCNN(exp_level=4, exponential_cutoff=3,expo_depth=2,expo_width=2,stage=stages[0],\n",
    "#                             prev_path=path, channels_width=256).cuda()\n",
    "#print(model.state_dict())\n",
    "#model.apply(init_kaim)\n",
    "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"Total trainable parameters:\", pytorch_total_params)\n",
    "# train(model,trainloader,epochs=10)\n",
    "# global_pruning(model,pruning_rate=0.3)\n",
    "# train(model,trainloader,epochs=10)\n",
    "# global_pruning(model,pruning_rate=0.5)\n",
    "# train(model,trainloader,epochs=10)\n",
    "# print(\"Training Accuracy\", validation(model,trainloader))\n",
    "# print(\"Val Accuracy:\",validation(model,val))\n",
    "# print(\"Test Accuracy: \",validation(model,test))\n",
    "# train(model,trainloader,epochs=10)\n",
    "# global_pruning(model,pruning_rate=0.7)\n",
    "# train(model,trainloader,epochs=20)\n",
    "# print(\"Training Accuracy\", validation(model,trainloader))\n",
    "# print(\"Val Accuracy:\",validation(model,val))\n",
    "# print(\"Test Accuracy: \",validation(model,test))\n",
    "\n",
    "# global_pruning(model,pruning_rate=0.3)\n",
    "# pruning_rate_manual = 0.5\n",
    "# global_pruning(model,pruning_rate=pruning_rate_manual)\n",
    "# print(\"Training Accuracy after pruning\", validation(model,trainloader))\n",
    "# print(\"Val Accuracy: after pruning\",validation(model,val))\n",
    "# #torch.save(model.state_dict(),path+\"/level4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87357,
     "status": "ok",
     "timestamp": 1594863440820,
     "user": {
      "displayName": "Jayant Parashar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjnbgSkOK4ntEuf3hVvbye4aBfehqWeRx41-5Ti=s64",
      "userId": "12884008915719439798"
     },
     "user_tz": 240
    },
    "id": "T9jh9oSMQ26s",
    "outputId": "42a10925-79f7-445c-95c3-352eacf3c911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 92.03\n",
      "Val Accuracy: 72.4\n",
      "Test Accuracy: 74.04\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy\", validation(model,trainloader))\n",
    "print(\"Val Accuracy:\",validation(model,val))\n",
    "print(\"Test Accuracy:\",validation(model,test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMiW6XWBW+73GMdNDfnq5PU",
   "collapsed_sections": [],
   "name": "CSNExperiments1:TestDeepCSN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05060518905f47e0811be4f9902085d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2778a85518ca4663bb3ab32803e5a4cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05060518905f47e0811be4f9902085d6",
      "placeholder": "​",
      "style": "IPY_MODEL_2b523bdd71034edbb975ebdb1669bc9c",
      "value": " 170500096/? [00:07&lt;00:00, 24005186.96it/s]"
     }
    },
    "2b523bdd71034edbb975ebdb1669bc9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e51336dfd454e3381fecc7277f3073d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0825529a2e1439281a0eb5a8fb2821a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32645aafcb794fa4bb3f55b5655228fe",
      "value": 1
     }
    },
    "32645aafcb794fa4bb3f55b5655228fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3a879c4707ec4572a3534b446a225a5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e51336dfd454e3381fecc7277f3073d",
       "IPY_MODEL_2778a85518ca4663bb3ab32803e5a4cc"
      ],
      "layout": "IPY_MODEL_e6514d71493647f6aa6ebe6ec373966b"
     }
    },
    "d0825529a2e1439281a0eb5a8fb2821a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6514d71493647f6aa6ebe6ec373966b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
